# Supervised and Unsupervised Machine Learning
This is the Supervised and Unsupervised Learning section of the [Algorithms in Machine Learning](https://supaerodatascience.github.io/syllabus.html#fsd311) class at ISAE-Supaero. The Neural Networks and Deep Learning topics have a [class of their own](https://supaerodatascience.github.io/deep-learning/) and won't be covered here.

* [Home](https://supaerodatascience.github.io/machine-learning/)
* [Github repository](https://github.com/SupaeroDataScience/machine-learning/)

## Syllabus

This course offers a discovery of the landscape of Machine Learning through some key algorithms.  
It follows the [Statistical Foundations of Machine Learning part](https://supaerodatascience.github.io/stat-ml) and will be followed by the [Neural Networks and Deep Learning part](https://supaerodatascience.github.io/deep-learning/).

Over 25 hours, we will cover the landscape of Supervised and Unsupervised Machine Learning by taking four different points of view and formalisms: the geometrical perspective, the probabilistic (Bayesian) perspective, the connectionnist perspective (which we will postpone to the next class on Deep Learning), and the ensemble perspective.  
The class is organized in 9 sessions:  

- Introduction.
    - Introduction to the landscape and workflow of Machine Learning, a few words on specificities of Unsupervised Learning, the importance of data preprocessing (1 session).
- A geometrical approach to Machine Learning.
    - Support Vector Machines, the bias/variance tradeoff and a bit of kernel theory. 
- A Bayesian perspective on Machine Learning.
    - Naive Bayes Classification and Gaussian Processes, 
    - followed by an in-depth class on Surrogate Modeling and Bayesian Optimization.
- Committee learning methods. 
    - Decision Trees and Boosting,
    - followed by an in-depth class on Gradient Boosting. 
    - Bagging and Random Forests,
    - followed by an in-depth class on anomaly detection.
- A "perspectives" final session.
    - Explainability in Machine Learning.

The pedagogy taken mixes voluntarily hands-on practice in Python with theoretical and mathematical understanding of the methods. At the end of the course you will be able to make an informed choice between the main families of ML algorithms, depending on the problem at hand. You will have an understanding of the algorithmic and mathematical properties of each family of methods and you will have a basic practical knowledge of the Scikit-Learn library.

## Bibliography

**The Elements of Statistical Learning.**  
T. Hastie, R. Tibshirani, J. Friedman.  
Springer Series in Statistics.  
[https://web.stanford.edu/~hastie/ElemStatLearn/](https://web.stanford.edu/~hastie/ElemStatLearn/)  

