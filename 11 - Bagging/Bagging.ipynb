{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc-sa/4.0/\"><img alt=\"Creative Commons License\" align=\"left\" src=\"https://i.creativecommons.org/l/by-nc-sa/4.0/80x15.png\" /></a>&nbsp;| [Emmanuel Rachelson](https://personnel.isae-supaero.fr/emmanuel-rachelson?lang=en) | <a href=\"https://supaerodatascience.github.io/machine-learning/\">https://supaerodatascience.github.io/machine-learning/</a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-size:22pt; line-height:25pt; font-weight:bold; text-align:center;\">Bagging</div>\n",
    "\n",
    "1. [The bootstrap (statistics)](#sec1)\n",
    "2. [Bootstraping in Machine Learning](#sec2)\n",
    "3. [Why does Bagging work?](#sec3)\n",
    "4. [Bagging in practice](#sec4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The general intuition:\n",
    "\n",
    "Trees are a higly variable predictor: slight changes in input data $\\Rightarrow$ huge changes in prediction.\n",
    "\n",
    "It seems rather like a weakness, let's use that as a strength.\n",
    "\n",
    "# <a id=\"sec1\"></a> 1. The bootstrap (statistics)\n",
    "\n",
    "Bootstrapping: a general non-parametric method in Statistics to estimate prediction variance.\n",
    "\n",
    "**The formal problem:**\n",
    "\n",
    "Suppose a set of examples $\\mathbf{x}$ of size $n$, where each component has been drawn from the same distribution $P_X$.<br>\n",
    "Suppose one wants to evaluate a statistic $\\theta$ of the random variable $X$, with an estimator $\\widehat{\\theta}$ over $\\mathbf{x}$.\n",
    "\n",
    "The estimator's variance across datasets $\\mathbf{x}$ of size $n$ is $Var(\\hat{\\theta}) = \\mathbb{E}_{\\mathbf{x}} \\left( \\hat{\\theta}^2 - \\mathbb{E}\\left(\\hat{\\theta}\\right)^2 \\right)$.\n",
    "\n",
    "We would like to estimate $\\widehat{\\theta}$'s distribution $P_{\\hat{\\theta}}$ over all possible training sets, in order (among other things) to assess if $\\widehat{\\theta}$ has a large estimation variance.\n",
    "\n",
    "This will inform us on the reliability of our estimate of $\\mathbb{E}(\\hat{\\theta})$ (over all possible training sets).\n",
    "\n",
    "To estimate this distribution $P_{\\hat{\\theta}}$, we need to have examples of various training sets of all possible sizes, which means one needs more data points in $\\mathbf{x}$, but such points are not available: we have $\\mathbf{x}$ and that's all.\n",
    "\n",
    "**Proposed solution:**\n",
    "\n",
    "- Simulate the distribution that generated $\\mathbf{x}$ by drawing $n$ samples from $\\mathbf{x}$ with replacement.\n",
    "- Evaluate $\\widehat{\\theta}$ on this new sample set of size $n$.\n",
    "- Repeat many times.\n",
    "- Approximately estimate $P_{\\hat{\\theta}}$ via the distribution $\\hat{P}_{\\hat{\\theta}}$ of the results.\n",
    "- (To estimate $\\mathbb{E}(\\hat{\\theta})$, compute the empirical average of the results.)\n",
    "\n",
    "**The algorithm:**\n",
    "\n",
    "For $b=1$ to $B$\n",
    "1. Bootstrap sample: obtain $\\mathbf{x}^b$ by sampling from $\\mathbf{x}$ with replacement.\n",
    "2. Compute $\\hat{\\theta}^b$ from $\\mathbf{x}^b$\n",
    "\n",
    "Estimate the distribution of $\\hat{\\theta}^b$.\n",
    "\n",
    "(Compute empirical average $\\hat{\\theta} = \\frac{1}{B} \\sum\\limits_{b=1}^B \\hat{\\theta}^b$)\n",
    "\n",
    "**Origin of the \"bootstrap\" name:**\n",
    "- Popular saying: to pull oneself up by one's own bootstraps.\n",
    "- Münchhausen (Rudolf E. Raspe)<br> pulls himself out of a mire by his own hair.\n",
    "- $[$French$]$ Méthode de Cyrano (E. Rostand)<br> Flies to the moon by 7 different ways, including throwing a magnet upwards.\n",
    "\n",
    "**Formally**\n",
    "\n",
    "Let $\\hat{P}_X$ be the *empirical distribution* of the $X$ variable (from the data $\\mathbf{x}$), and $\\hat{F}_X$ its *empirical cumulative distribution function*.\n",
    "\n",
    "As $n\\rightarrow \\infty$, $\\hat{F}_X$ converges to the true cumulative distribution function $F_X$ of $X$.\n",
    "\n",
    "We draw from $\\hat{F}_X$ to simulate the true $F_X$.\n",
    "\n",
    "This provides us with $\\hat{P}_{\\hat{\\theta}}$ , which we assume to be representative to $P_{\\hat{\\theta}}$.\n",
    "\n",
    "References:<br>\n",
    "**An Introduction to the Bootstrap.**<br>\n",
    "B. Efron and R. Tibshirani, Chapman & Hall/CRC, (1993).\n",
    "\n",
    "**[What Teachers Should Know About the Bootstrap: Resampling in the Undergraduate Statistics Curriculum](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4784504/)**<br>\n",
    "T. C. Hesterberg, *The American Statistician*, 69(4), 371–386, (2015).<br>\n",
    "[arXiv longer version](https://arxiv.org/abs/1411.5279)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's practice on an example. In 1878, Simon Newcomb took observations on the speed of light. The data set below (taken from the [Bayesian data analysis book datasets](http://www.stat.columbia.edu/~gelman/book/)) describes the deviations in travel time from $24800$ nanoseconds. This data contains two outliers, which influence the sample mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "times = np.array([28, 26, 33, 24, 34, -44, 27, 16, 40, -2, 29,\n",
    "                  22, 24, 21, 25, 30, 23, 29, 31, 19, 24, 20,\n",
    "                  36, 32, 36, 28, 25, 21, 28, 29, 37, 25, 28,\n",
    "                  26, 30, 32, 36, 26, 30, 22, 36, 23, 27, 27,\n",
    "                  28, 27, 31, 27, 26, 33, 26, 32, 32, 24, 39,\n",
    "                  28, 24, 25, 32, 25, 29, 27, 28, 29, 16, 23])\n",
    "\n",
    "print(\"Number of observations:\", times.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.hist(times, bins=np.arange(np.min(times),np.max(times),1));\n",
    "plt.title(\"Travel times distribution\")\n",
    "plt.show()\n",
    "print(\"Sample mean:\", np.mean(times))\n",
    "print(\"Sample median:\", np.median(times))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "**Exercice:**<br>\n",
    "Implement a Bootstrap algorithm to estimate the distribution of the empirical average and empirical median estimators on this data.<br>\n",
    "Plot the histogram of these distributions (use [`plt.hist`](https://matplotlib.org/api/_as_gen/matplotlib.pyplot.hist.html)).<br>\n",
    "Estimate the estimator's empirical average from the bootstrap samples and the mean of its distribution.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/code1.py\n",
    "### WRITE YOUR CODE HERE\n",
    "# If you get stuck, uncomment the line above to load a correction in this cell (then you can execute this code)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that:\n",
    "- the Bootstrap does not provide better estimates of the statistic than the original estimator,\n",
    "- however it provides *distribution* estimates (confidence intervals) and thus can help reduce variance."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id=\"sec2\"></a> 2. Bootstraping in Machine Learning\n",
    "\n",
    "Training set: $\\mathcal{T}=\\left\\{(x_i,y_i)\\right\\}_{i=1..N}$.<br>\n",
    "Predictor $\\varphi(x)$ trained on $\\mathcal{T}$.\n",
    "\n",
    "Bootstrap replicates of $\\mathcal{T}$ to build a better predictor $\\varphi_B$?<br>\n",
    "$\\rightarrow$ Idea of Bagging (**B**ootstrap **agg**regat**ing**)\n",
    "\n",
    "**Bagging in a nutshell:**\n",
    "\n",
    "For $b=1$ to $B$\n",
    "1. Take bootstrap replicate $\\mathcal{T}^b$ of $\\mathcal{T}$\n",
    "2. Train $\\varphi^b$ on $\\mathcal{T}^b$\n",
    "\n",
    "Return $\\varphi_B: x\\mapsto \\arg\\max\\limits_{j} \\sum\\limits_{b=1}^B I(\\varphi^b(x) = j)$ (majority vote).\n",
    "\n",
    "In the regression case: $\\varphi_B: x\\mapsto \\frac{1}{B} \\sum\\limits_{b=1}^B \\varphi^b(x)$.\n",
    "\n",
    "The parallel between Bagging and the Bootstrap:\n",
    "\n",
    "$$\\begin{array}{rcl}\n",
    "\\mathbf{x}=(x_1,\\ldots,x_N) & \\leftrightarrow & \\mathcal{T} = \\left\\{(x_i,y_i)\\right\\}_{i=1..N}\\\\\n",
    "\\mathbf{x}^b=(x^b_1,\\ldots,x^b_N) & \\leftrightarrow & \\mathcal{T}^b =\\left\\{(x^b_i,y^b_i)\\right\\}_{i=1..N}\\\\\n",
    "\\textrm{empirical distribution }\\hat{P}_X & \\leftrightarrow & \\textrm{empirical distribution } \\hat{P}_\\mathcal{T}\\\\\n",
    "\\textrm{true distribution }P_X & \\leftrightarrow & \\textrm{true distribution } P_\\mathcal{T}\\\\\n",
    "\\hat{\\theta}^b & \\leftrightarrow & \\varphi^b\\\\\n",
    "\\mathbb{E}_{\\hat{P}_{\\hat{\\theta}}}\\left(\\hat{\\theta}\\right)=\\frac{1}{B}\\sum \\hat{\\theta}^b & \\leftrightarrow & \\varphi_B\\\\\n",
    "\\mathbb{E}_{P_{\\hat{\\theta}}}\\left(\\hat{\\theta}\\right) & \\leftrightarrow & \\varphi_A(\\cdot)=\\mathbb{E}_{P_\\mathcal{T}}\\left(\\varphi_\\mathcal{T}(\\cdot)\\right)\\\\\n",
    "\\theta & \\leftrightarrow & \\mathbb{E}_X(Y|X)\n",
    "\\end{array}$$\n",
    "\n",
    "The (Bootstrap) key idea:<br>\n",
    "Since, with enough samples $N$, $\\varphi_A(x) = \\mathbb{E}_{P_\\mathcal{T}}\\left(\\varphi_\\mathcal{T}(x)\\right)$ converges to the optimal predictor of $\\mathbb{E}_X(Y|X)$, we build the Bootstrap distribution over $\\mathcal{T}^b$ to obtain $\\varphi_B$ and we use $\\varphi_B$ to approximate $\\varphi_A$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id=\"sec3\"></a> 3. Why does Bagging work?\n",
    "\n",
    "Let:\n",
    "- $\\varphi(\\cdot)$ be the base classifier<br>\n",
    "(generalization error $e$).\n",
    "- $\\varphi_B(\\cdot) = \\mathbb{E}_{\\hat{\\mathcal{T}}} \\left( \\varphi(\\cdot) \\right)$ be the Bagging estimator built with Bootstrap samples<br>\n",
    "(generalization error $e_B$).\n",
    "- $\\varphi_A(\\cdot) = \\mathbb{E}_{\\mathcal{T}} \\left( \\varphi(\\cdot) \\right)$ be the aggregate estimator averaging over all possible training sets<br>\n",
    "(generalization error $e_A$).\n",
    "\n",
    "In practice, we never get $\\varphi_A$.\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "\n",
    "**Key results:**<br>\n",
    "*1st result:* $\\varphi_A$ is always at least as good as $\\varphi$; $e_A \\leq e$<br>\n",
    "<br>\n",
    "*2nd result:* The highest the variance of $\\varphi$ across training sets $\\mathcal{T}$, the more improvement $\\varphi_A$ produces.<br>\n",
    "<br>\n",
    "*3rd result:* $\\varphi_B$ only approximates $\\varphi_A$ so $e_A \\leq e_B$\n",
    "</div>\n",
    "\n",
    "Consequences:\n",
    "- If $\\varphi$ highly variable w.r.t. $\\mathcal{T}$, $\\varphi_B$ improves on $\\varphi$ through aggregation.\n",
    "- But if $\\varphi$ is rather stable w.r.t. $\\mathcal{T}$, $e_A\\approx e$ and since $\\varphi_B$ approximates $\\varphi_A$, $e_B$ might be greater than $e$.\n",
    "\n",
    "For a more rigorous explanation, see the attached document <a href=\"WhyBaggingWorks.pdf\">WhyBaggingWorks.pdf</a>.\n",
    "\n",
    "**So it does not always work?**\n",
    "\n",
    "Actually, no, it does not always work.<br>\n",
    "Bagging should be used to transform highly variable predictors $\\varphi$ into a more accurate averaged commitee $\\varphi_B$.\n",
    "\n",
    "Examples of $\\varphi$ that Bagging improves:<br>\n",
    "$\\rightarrow$ Trees, Neural Networks.<br>\n",
    "Examples of $\\varphi$ that Bagging does not improve much (or degrades):<br>\n",
    "$\\rightarrow$ Support Vector Machines, Gaussian Processes."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id=\"sec4\"></a> 4. Bagging in practice\n",
    "\n",
    "Let's study the decision boundary and the generalization error of a Forest of Trees obtained with Bagging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "def gen_data(seed):\n",
    "    X1, y1 = datasets.make_gaussian_quantiles(cov=2.,\n",
    "                                 n_samples=300, n_features=2,\n",
    "                                 n_classes=2, random_state=seed)\n",
    "    X2, y2 = datasets.make_gaussian_quantiles(mean=(3, 3), cov=1.5,\n",
    "                                 n_samples=700, n_features=2,\n",
    "                                 n_classes=2, random_state=seed)\n",
    "    X = np.concatenate((X1, X2))\n",
    "    y = np.concatenate((y1, - y2 + 1))\n",
    "    y = 2*y-1\n",
    "    X, y = shuffle(X, y)\n",
    "    return X, y\n",
    "\n",
    "X,y = gen_data(1)\n",
    "Xtest,X = np.split(X,[400])\n",
    "ytest,y = np.split(y,[400])\n",
    "\n",
    "Xblue = X[y==-1]\n",
    "Xred = X[y==1]\n",
    "fig=plt.figure(figsize=(8, 8), dpi= 80, facecolor='w', edgecolor='k')\n",
    "plt.scatter(Xblue[:,0],Xblue[:,1],c='b')\n",
    "plt.scatter(Xred[:,0],Xred[:,1],c='r');"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's introduce three utility functions:\n",
    "- `forest_predict` takes a forest as a list of trees and a vector of inputs, and returns the majority vote of the forest for every input.\n",
    "- `forest_score` takes a forest, a vector of samples and labels and returns the classification accuracy of the forest.\n",
    "- `plot_decision_boundary_forest` takes a forest and a set of labeled points and plots the points and the decision boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "from sklearn.utils import resample\n",
    "\n",
    "### Utility functions\n",
    "def forest_predict(f, X):\n",
    "    N = len(f)\n",
    "    votes = np.zeros((X.shape[0],N))\n",
    "    for i in range(N):\n",
    "        votes[:,i] = f[i].predict(X)\n",
    "    return np.sign(np.sum(votes,axis=1))\n",
    "\n",
    "def forest_score(f, X, y):\n",
    "    n=len(y)\n",
    "    return np.sum(np.not_equal(forest_predict(f,X),y))/n\n",
    "\n",
    "def plot_decision_boundary_forest(f, X, y):\n",
    "    plot_step = 0.02\n",
    "    x0_min, x0_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    x1_min, x1_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx0, xx1 = np.meshgrid(np.arange(x0_min, x0_max, plot_step), np.arange(x1_min, x1_max, plot_step))\n",
    "    yypred = forest_predict(f, np.c_[xx0.ravel(),xx1.ravel()])\n",
    "    yypred = yypred.reshape(xx0.shape)\n",
    "    plt.contourf(xx0, xx1, yypred, cmap=plt.cm.Paired)\n",
    "    y_pred = forest_predict(f, X)\n",
    "    Xblue_good = X[np.equal(y,-1)*np.equal(y,y_pred)]\n",
    "    Xblue_bad  = X[np.equal(y,-1)*np.not_equal(y,y_pred)]\n",
    "    Xred_good  = X[np.equal(y,1)*np.equal(y,y_pred)]\n",
    "    Xred_bad   = X[np.equal(y,1)*np.not_equal(y,y_pred)]\n",
    "    plt.scatter(Xblue_good[:,0],Xblue_good[:,1],c='b')\n",
    "    plt.scatter(Xblue_bad[:,0],Xblue_bad[:,1],c='c',marker='v')\n",
    "    plt.scatter(Xred_good[:,0],Xred_good[:,1],c='r')\n",
    "    plt.scatter(Xred_bad[:,0],Xred_bad[:,1],c='m',marker='v')\n",
    "    plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's re-generate some training and testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Generate data\n",
    "X,y = gen_data(1)\n",
    "Xtest,X = np.split(X,[400])\n",
    "ytest,y = np.split(y,[400])\n",
    "\n",
    "Xblue = X[y==-1]\n",
    "Xred = X[y==1]\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.scatter(Xblue[:,0],Xblue[:,1],c='b')\n",
    "plt.scatter(Xred[:,0],Xred[:,1],c='r');"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "**Exercice:**<br>\n",
    "Implement a Bagging procedure that builds a forest of 101 trees.<br>\n",
    "Monitor the training and generalization error of individual trees and of the forest, along the forest growth.<br>\n",
    "Display and comment.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "from sklearn.utils import resample\n",
    "\n",
    "### Growing the forest\n",
    "Nbootstrap = 101\n",
    "forest = list()\n",
    "tree_training_error = np.zeros(Nbootstrap)\n",
    "tree_generalization_error = np.zeros(Nbootstrap)\n",
    "forest_training_error = np.zeros(Nbootstrap)\n",
    "forest_generalization_error = np.zeros(Nbootstrap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/code2.py\n",
    "### WRITE YOUR CODE HERE\n",
    "# If you get stuck, uncomment the line above to load a correction in this cell (then you can execute this code)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Display\n",
    "print(\"Standard deviation of single tree training error: %g\"%np.std(tree_training_error))\n",
    "print(\"Standard deviation of single tree generalization error: %g\"%np.std(tree_generalization_error))\n",
    "print(\"Standard deviation of forest generalization error on last 15 iterations: %g\"\n",
    "      %np.std(forest_generalization_error[-15:]))\n",
    "print(\"Average forest generalization error on last 15 iterations: %g\"\n",
    "      %np.mean(forest_generalization_error[-15:]))\n",
    "print(\"Last forest generalization error on last 15 iterations: %g\"%forest_generalization_error[-1])\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.plot(forest_training_error,c='r')\n",
    "plt.plot(forest_generalization_error,c='g')\n",
    "plt.figure(figsize=(8,8))\n",
    "plot_decision_boundary_forest(forest,X,y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
