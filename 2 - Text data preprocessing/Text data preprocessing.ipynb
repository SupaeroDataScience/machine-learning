{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc-sa/4.0/\"><img alt=\"Creative Commons License\" align=\"left\" src=\"https://i.creativecommons.org/l/by-nc-sa/4.0/80x15.png\" /></a>&nbsp;| [Emmanuel Rachelson](https://personnel.isae-supaero.fr/emmanuel-rachelson?lang=en) | <a href=\"https://supaerodatascience.github.io/machine-learning/\">https://supaerodatascience.github.io/machine-learning/</a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-size:22pt; line-height:25pt; font-weight:bold; text-align:center;\">Text data pre-processing</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercice, we shall load a database of email messages and pre-format them so that we can design automated classification methods or use off-the-shelf classifiers. The general purpose of this notebook is to give a practical notion (through this example) of how important data pre-processing can be in a Machine Learning workflow, and generalize it to other situations.\n",
    "\n",
    "\"What is there to pre-process?\" you might ask. Well, actually, text data comes in a very noisy form that we, humans, have become accustomed to and filter out effortlessly to grasp the core meaning of the text. It has a lot of formatting (fonts, colors, typography...), punctuation, abbreviations, common words, grammatical rules, etc. that we might wish to discard before even starting the data analysis.\n",
    "\n",
    "Here are some pre-processing steps that can be performed on text:\n",
    "1. loading the data, removing attachements, merging title and body;\n",
    "2. tokenizing - splitting the text into atomic \"words\";\n",
    "3. removal of stop-words - very common words;\n",
    "4. removal of non-words - punctuation, numbers, gibberish;\n",
    "3. lemmatization - merge together \"find\", \"finds\", \"finder\".\n",
    "\n",
    "The final goal is to be able to represent a document as a mathematical object, e.g. a vector, that our machine learning black boxes can process."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<img id=\"fig1\" src=\"https://imgs.xkcd.com/comics/constructive.png\"> \n",
    "\n",
    "A tech company comes to you to create a moderation system for their social network : they want to detect spam comments, and later on also detect offensive contents to remove them automatically.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#1.-Load-the-data\" data-toc-modified-id=\"1.-Load-the-data-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>1. Load the data</a></span></li><li><span><a href=\"#2.-Filtering-out-the-noise\" data-toc-modified-id=\"2.-Filtering-out-the-noise-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>2. Filtering out the noise</a></span></li><li><span><a href=\"#3.-Even-better-filtering\" data-toc-modified-id=\"3.-Even-better-filtering-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>3. Even better filtering</a></span></li><li><span><a href=\"#4.-Term-frequency-times-inverse-document-frequency\" data-toc-modified-id=\"4.-Term-frequency-times-inverse-document-frequency-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>4. Term frequency times inverse document frequency</a></span></li><li><span><a href=\"#5.-Utility-function\" data-toc-modified-id=\"5.-Utility-function-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>5. Utility function</a></span></li></ul></div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load the data\n",
    "\n",
    "To showcase our proposed system, we load a database of email messages and pre-format them so that we can design automated classification methods or use off-the-shelf classifiers.\n",
    "\n",
    "\n",
    "**Questions** :\n",
    "- What simple statistics could you print on the dataset ?\n",
    "- Why is there multiple folders on the dataset ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!git clone git@github.com:SupaeroDataScience/machine-learning.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of emails 2893\n",
      "email file: ../data/lingspam_public/bare/part6/9-1699msg2.txt\n",
      "email is a spam: False\n",
      "Subject: 7th european nlg workshop\n",
      "\n",
      "7th european workshop on natural language generation call for papers toulouse ( france ) , may 13-15 1999 this seventh european workshop on natural language generation will focus on all aspects of computational linguistics and industrial applications related to natural language generation . papers related to theoretical aspects , applied research and ongoing projects are encouraged . natural language generation being the study of a number of models , approaches and systems , the workshop will include , besides classical topics , scientific domains in which natural language generation plays an important role , such as speech , dialogue , multi-media interfaces , psycho-linguistics , and theoretical linguistics . relevant application domains include all types of applications ( mt , ir and ie , etc . ) in which generation plays an important role and where specific techniques or models of generation have been developed . the main topics are the following , without excluding others : - lexical aspects : phonology , morphology , syntax and semantics , - syntactic aspects , - semantic and pragmatic aspects , - speech synthesis and nlg , oral dialogue , - construction of knowledge bases for nlg , - applications of nlg : mt , summarization , report generation , etc . , - multi-media generation including graphics , numerical information , and texts in various formats , etc . , - psycholinguistic aspects of nlg : in speech production , in discourse production and management , in lexicalization , etc . - architectures for nlg , - internet and web applications using nlg . multiple submissions must be mentioned , and if the paper is accepted at several places , presentation at the workshop will be conditional to the paper been withdrawn from these other places . papers should be prepared in latex ( preferably ) or in word ( send rtf file ) , not exceeding 4000 words ( about 8 to 10 pages long ) , including references . more details about format submissions will be given soon on the workshop web site . papers must relate original , unpublished work . work in progress can also be submitted . papers must include the authors ' name , full address and e-mail . they will be reviewed anonymously . therefore , a title page must come separately , with the title of the paper , the abstract , the authors ' names and addresses , and , if appropriate , the mention of multiple submissions ( and where the paper has been submitted ) . no indication of the authors ' identity must appear in the text of the paper . deadlines : january 25th submission of papers by e-mail march 10th notification of acceptance / rejection april 15th final paper due ( paper copy ) may 13-15 workshop papers and all correspondence should be sent to stdizier @ irit . fr programme committee : christy doran wolfgang hoeppner helmut horacek eduard hovy guy lapalme kathy mccoy david mcdonald kathy mckeown chris mellish cecile paris patrick saint - dizier manfred stede michael zock local organization information : web site ( forthcoming ) : http : / / www . irit . fr / manifs / manifs . html look for : ewnlg ' 99 . the meeting will be held in toulouse downtown , in the holiday inn hotel , on the famous ` place du capitole ' , in the heart of the old town . toulouse has an international airport , with many national and european destinations . there are many places worth visiting in the city , all within about 10 minutes walking distance ( museums , old roman - style churches , 16th - 17th century private houses with inner yards , etc . ) . fees should be around 750 french francs ( about 130 us $ ) , for 2 days , including two lunches , the breaks and the proceedings . for three days , fees will be about 1000f . blocks of rooms will be reserved in the hotel meeting ( 530f per room per day , incl . buffet breakfast ) and in other , cheaper , hotels around at preferential rates .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "train_dir = '../data/lingspam_public/bare/'\n",
    "# train_dir = 'machine-learning/data/ling-spam/train-mails/'\n",
    "\n",
    "email_path = []\n",
    "email_label = []\n",
    "for d in os.listdir(train_dir):\n",
    "    folder = os.path.join(train_dir,d)\n",
    "    email_path += [os.path.join(folder,f) for f in os.listdir(folder)]\n",
    "    email_label += [f[0:3]=='spm' for f in os.listdir(folder)]\n",
    "print(\"number of emails\",len(email_path))\n",
    "email_nb = 8 # try 8 for a spam example\n",
    "print(\"email file:\", email_path[email_nb])\n",
    "print(\"email is a spam:\", email_label[email_nb])\n",
    "print(open(email_path[email_nb]).read())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Filtering out the noise\n",
    "\n",
    "One nice thing about scikit-learn is that is has lots of preprocessing utilities. Like [`CountVectorizer`](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) for instance, that converts a collection of text documents to a matrix of token counts.\n",
    "\n",
    "- To remove stop-words, we set: `stop_words='english'`\n",
    "- To convert all words to lowercase: `lowercase=True`\n",
    "- The default tokenizer in scikit-learn removes punctuation and only keeps words of more than 2 letters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "countvect = CountVectorizer(input='filename', stop_words='english', lowercase=True)\n",
    "word_count = countvect.fit_transform(email_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 2893\n",
      "Number of words: 60618\n",
      "Document - words matrix: (2893, 60618)\n",
      "First words: ['00' '000' '0000' '00001' '00003000140' '00003003958' '00007' '0001'\n",
      " '00010' '00014' '0003' '00036' '000bp' '000s' '000yen' '001' '0010'\n",
      " '0010010034' '0011' '00133' '0014' '00170' '0019' '00198' '002' '002656'\n",
      " '0027' '003' '0030' '0031' '00333' '0037' '0039' '003n7' '004' '0041'\n",
      " '0044' '0049' '005' '0057' '006' '0067' '007' '00710' '0073' '0074'\n",
      " '00799' '008' '009' '00919680' '0094' '00a' '00am' '00arrival' '00b'\n",
      " '00coffee' '00congress' '00d' '00dinner' '00f' '00h' '00hfstahlke' '00i'\n",
      " '00j' '00l' '00m' '00p' '00pm' '00r' '00t' '00tea' '00the' '00uzheb' '01'\n",
      " '0100' '01003' '01006' '0104' '0106' '01075' '0108' '011' '0111' '0117'\n",
      " '0118' '01202' '01222' '01223' '01225' '01232' '01235' '01273' '013'\n",
      " '0131' '01334' '0135' '01364' '0139' '013953' '013a']\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of documents:\", len(email_path))\n",
    "words = countvect.get_feature_names_out()\n",
    "print(\"Number of words:\", len(words))\n",
    "print(\"Document - words matrix:\", word_count.shape)\n",
    "print(\"First words:\", words[0:100])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Even better filtering\n",
    "\n",
    "That's already quite ok, but this pre-processing does not perform lemmatization, the list of stop-words could be better and we could wish to remove non-english words (misspelled, with numbers, etc.).\n",
    "\n",
    "A slightly better preprocessing uses the [Natural Language Toolkit](https://www.nltk.org/https://www.nltk.org/). The one below:\n",
    "- tokenizes;\n",
    "- removes punctuation;\n",
    "- removes stop-words;\n",
    "- removes non-English and misspelled words (optional);\n",
    "- removes 1-character words;\n",
    "- removes non-alphabetical words (numbers and codes essentially)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /home/yann/py_env/sdd_env/lib/python3.10/site-packages (3.8.1)\n",
      "Requirement already satisfied: joblib in /home/yann/py_env/sdd_env/lib/python3.10/site-packages (from nltk) (1.3.2)\n",
      "Requirement already satisfied: click in /home/yann/py_env/sdd_env/lib/python3.10/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/yann/py_env/sdd_env/lib/python3.10/site-packages (from nltk) (2023.8.8)\n",
      "Requirement already satisfied: tqdm in /home/yann/py_env/sdd_env/lib/python3.10/site-packages (from nltk) (4.66.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/yann/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package words to /home/yann/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/yann/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/yann/nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run only of nltk is not installed\n",
    "%pip install nltk\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('words')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yann/py_env/sdd_env/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from nltk import wordpunct_tokenize          \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import words\n",
    "from string import punctuation\n",
    "class LemmaTokenizer(object):\n",
    "    def __init__(self, remove_non_words=True):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "        self.stopwords = set(stopwords.words('english'))\n",
    "        self.words = set(words.words())\n",
    "        self.remove_non_words = remove_non_words\n",
    "    def __call__(self, doc):\n",
    "        # tokenize words and punctuation\n",
    "        word_list = wordpunct_tokenize(doc)\n",
    "        # remove stopwords\n",
    "        word_list = [word for word in word_list if word not in self.stopwords]\n",
    "        # remove non words\n",
    "        if(self.remove_non_words):\n",
    "            word_list = [word for word in word_list if word in self.words]\n",
    "        # remove 1-character words\n",
    "        word_list = [word for word in word_list if len(word)>1]\n",
    "        # remove non alpha\n",
    "        word_list = [word for word in word_list if word.isalpha()]\n",
    "        return [self.wnl.lemmatize(t) for t in word_list]\n",
    "\n",
    "countvect = CountVectorizer(input='filename',tokenizer=LemmaTokenizer(remove_non_words=True))\n",
    "word_count = countvect.fit_transform(email_path)\n",
    "feat2word = {v: k for k, v in countvect.vocabulary_.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 2893\n",
      "Number of words: 14279\n",
      "Document - words matrix: (2893, 14279)\n",
      "First words: ['aa' 'aal' 'aba' 'aback' 'abacus' 'abandon' 'abandoned' 'abandonment'\n",
      " 'abbas' 'abbreviation' 'abdomen' 'abduction' 'abed' 'aberrant'\n",
      " 'aberration' 'abide' 'abiding' 'abigail' 'ability' 'ablative' 'ablaut'\n",
      " 'able' 'abler' 'aboard' 'abolition' 'abord' 'aboriginal' 'aborigine'\n",
      " 'abound' 'abox' 'abreast' 'abridged' 'abroad' 'abrogate' 'abrook'\n",
      " 'abruptly' 'abscissa' 'absence' 'absent' 'absolute' 'absolutely'\n",
      " 'absoluteness' 'absolutist' 'absolutive' 'absolutization' 'absorbed'\n",
      " 'absorption' 'abstract' 'abstraction' 'abstractly' 'abstractness'\n",
      " 'absurd' 'absurdity' 'abu' 'abundance' 'abundant' 'abuse' 'abusive'\n",
      " 'abyss' 'academe' 'academic' 'academically' 'academician' 'academy'\n",
      " 'accelerate' 'accelerated' 'accelerative' 'accent' 'accentuate'\n",
      " 'accentuation' 'accept' 'acceptability' 'acceptable' 'acceptance'\n",
      " 'acceptation' 'accepted' 'acception' 'access' 'accessibility'\n",
      " 'accessible' 'accessibly' 'accidence' 'accident' 'accidental'\n",
      " 'accidentality' 'accidentally' 'acclaim' 'accommodate' 'accommodation'\n",
      " 'accompany' 'accomplish' 'accomplished' 'accomplishment' 'accord'\n",
      " 'accordance' 'according' 'accordingly' 'account' 'accountability'\n",
      " 'accountant']\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of documents:\", len(email_path))\n",
    "words = countvect.get_feature_names_out()\n",
    "print(\"Number of words:\", len(words))\n",
    "print(\"Document - words matrix:\", word_count.shape)\n",
    "print(\"First words:\", words[0:100])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Term frequency times inverse document frequency\n",
    "\n",
    "After this first preprocessing, each document is summarized by a vector of size \"number of words in the extracted dictionnary\". For example, the first email in the list has become:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original email:\n",
      "Subject: theoretical , descriptive , and applied linguistics\n",
      "\n",
      "call for papers : last announcement kentucky foreign language conference linguistics sessions the 52nd annual kentucky foreign language conference will be held april 22-24 , 1999 , at the university of kentucky in lexington . the conference will include sessions devoted to all aspects of theoretical and descriptive linguistics , sociolinguistics , and applied linguistics . if you wish to present a paper in one of these sessions , send two copies of a one page abstract to prof . anna bosch , 1215 patterson office tower , university of kentucky , lexington , ky 40506-0027 . alternatively , send your abstract by email to : bosch @ pop . uky . edu . ( email submissions are encouraged . ) please include the following information with your abstract : name , affiliation , address , email address , daytime phone . the deadline for submission of abstracts is monday , november 16 , 1998 . authors will be notified about the conference schedule in mid - december . web page : www . uky . edu / artssciences / kflc\n",
      "\n",
      "Bag of words representation (45 words in dict):\n",
      "{'subject': 1, 'paper': 1, 'phone': 1, 'one': 2, 'two': 1, 'information': 1, 'include': 2, 'university': 2, 'theoretical': 2, 'descriptive': 2, 'applied': 2, 'linguistics': 4, 'call': 1, 'last': 1, 'announcement': 1, 'foreign': 2, 'language': 2, 'conference': 4, 'session': 3, 'annual': 1, 'devoted': 1, 'wish': 1, 'present': 1, 'send': 2, 'page': 2, 'abstract': 3, 'prof': 1, 'anna': 1, 'bosch': 2, 'office': 1, 'tower': 1, 'alternatively': 1, 'pop': 1, 'please': 1, 'following': 1, 'name': 1, 'affiliation': 1, 'address': 2, 'daytime': 1, 'deadline': 1, 'submission': 1, 'notified': 1, 'schedule': 1, 'mid': 1, 'web': 1}\n",
      "\n",
      "Vector reprensentation (45 non-zero elements):\n",
      "  (0, 12153)\t1\n",
      "  (0, 8854)\t1\n",
      "  (0, 9193)\t1\n",
      "  (0, 8596)\t2\n",
      "  (0, 13170)\t1\n",
      "  (0, 6390)\t1\n",
      "  (0, 6244)\t2\n",
      "  (0, 13427)\t2\n",
      "  (0, 12726)\t2\n",
      "  (0, 3342)\t2\n",
      "  (0, 658)\t2\n",
      "  (0, 7261)\t4\n",
      "  (0, 1733)\t1\n",
      "  (0, 7038)\t1\n",
      "  (0, 559)\t1\n",
      "  (0, 4956)\t2\n",
      "  (0, 7019)\t2\n",
      "  (0, 2543)\t4\n",
      "  (0, 11274)\t3\n",
      "  (0, 564)\t1\n",
      "  (0, 3415)\t1\n",
      "  (0, 14119)\t1\n",
      "  (0, 9659)\t1\n",
      "  (0, 11220)\t2\n",
      "  (0, 8796)\t2\n",
      "  (0, 47)\t3\n",
      "  (0, 9771)\t1\n",
      "  (0, 552)\t1\n",
      "  (0, 1483)\t2\n",
      "  (0, 8566)\t1\n",
      "  (0, 12936)\t1\n",
      "  (0, 437)\t1\n",
      "  (0, 9449)\t1\n",
      "  (0, 9344)\t1\n",
      "  (0, 4923)\t1\n",
      "  (0, 8176)\t1\n",
      "  (0, 275)\t1\n",
      "  (0, 173)\t2\n",
      "  (0, 3104)\t1\n",
      "  (0, 3109)\t1\n",
      "  (0, 12170)\t1\n",
      "  (0, 8439)\t1\n",
      "  (0, 11060)\t1\n",
      "  (0, 7824)\t1\n",
      "  (0, 13971)\t1\n"
     ]
    }
   ],
   "source": [
    "mail_number = 1\n",
    "text = open(email_path[mail_number]).read()\n",
    "print(\"Original email:\")\n",
    "print(text)\n",
    "#print(LemmaTokenizer()(text))\n",
    "#print(len(set(LemmaTokenizer()(text))))\n",
    "#print(len([feat2word[i] for i in word_count2[mail_number, :].nonzero()[1]]))\n",
    "#print(len([word_count2[mail_number, i] for i in word_count2[mail_number, :].nonzero()[1]]))\n",
    "#print(set([feat2word[i] for i in word_count2[mail_number, :].nonzero()[1]])-set(LemmaTokenizer()(text)))\n",
    "emailBagOfWords = {feat2word[i]: word_count[mail_number, i] for i in word_count[mail_number, :].nonzero()[1]}\n",
    "print(\"Bag of words representation (\", len(emailBagOfWords), \" words in dict):\", sep='')\n",
    "print(emailBagOfWords)\n",
    "print(\"\\nVector reprensentation (\", word_count[mail_number, :].nonzero()[1].shape[0], \" non-zero elements):\", sep='')\n",
    "print(word_count[mail_number, :])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions** : \n",
    "- What is a bag-of-word representation ?\n",
    "- What kind of feature selection or feature engineering could you derivate from this bag of word representation ?\n",
    "\n",
    "Counting words is a good start but there is an issue: longer documents will have higher average count values than shorter documents, even though they might talk about the same topics.\n",
    "\n",
    "To avoid these potential discrepancies it suffices to divide the number of occurrences of each word in a document by the total number of words in the document: these new features are called `tf` for Term Frequencies.\n",
    "\n",
    "Another refinement on top of `tf` is to downscale weights for words that occur in many documents in the corpus and are therefore less informative than those that occur only in a smaller portion of the corpus.\n",
    "\n",
    "This downscaling is called `tf–idf` for “Term Frequency times Inverse Document Frequency” and again, scikit-learn does the job for us with the [TfidfTransformer](scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html) function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2893, 14279)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf = TfidfTransformer().fit_transform(word_count)\n",
    "tfidf.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now every email in the corpus has a vector representation that filters out unrelevant tokens and retains the significant information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "email 0:\n",
      "  (0, 14243)\t0.02575565818614938\n",
      "  (0, 14188)\t0.009037157386654272\n",
      "  (0, 14184)\t0.018914479293583534\n",
      "  (0, 14165)\t0.029251285089143514\n",
      "  (0, 14129)\t0.027374168199134727\n",
      "  (0, 14128)\t0.02347142436978794\n",
      "  (0, 14095)\t0.021020615003509328\n",
      "  (0, 14000)\t0.034397003844170014\n",
      "  (0, 13952)\t0.022218258449752018\n",
      "  (0, 13907)\t0.0651513065186056\n",
      "  (0, 13818)\t0.027708809423839673\n",
      "  (0, 13807)\t0.014715215692291669\n",
      "  (0, 13799)\t0.10909159029227081\n",
      "  (0, 13758)\t0.01477025767369468\n",
      "  (0, 13741)\t0.013254943526520086\n",
      "  (0, 13699)\t0.026618526166468353\n",
      "  (0, 13664)\t0.019505300856334735\n",
      "  (0, 13613)\t0.04002600677129189\n",
      "  (0, 13427)\t0.01395802627701826\n",
      "  (0, 13310)\t0.033543417774697756\n",
      "  (0, 13190)\t0.009744913287464005\n",
      "  (0, 13170)\t0.009722925887796722\n",
      "  (0, 13141)\t0.02300714480492907\n",
      "  (0, 13140)\t0.01748772028398016\n",
      "  (0, 13114)\t0.01839729642565856\n",
      "  :\t:\n",
      "  (0, 1302)\t0.05546000340672882\n",
      "  (0, 1257)\t0.012922317342736063\n",
      "  (0, 1085)\t0.02631056242208824\n",
      "  (0, 1084)\t0.1078046094010095\n",
      "  (0, 1081)\t0.10210823575034515\n",
      "  (0, 1079)\t0.034397003844170014\n",
      "  (0, 1034)\t0.061428338392916275\n",
      "  (0, 818)\t0.07074373071933382\n",
      "  (0, 795)\t0.015886478832391346\n",
      "  (0, 774)\t0.01721286913585253\n",
      "  (0, 683)\t0.023738077613794806\n",
      "  (0, 656)\t0.030314210117875617\n",
      "  (0, 646)\t0.016893049152780927\n",
      "  (0, 614)\t0.013030261303721117\n",
      "  (0, 611)\t0.029191010974761785\n",
      "  (0, 570)\t0.016400170080862222\n",
      "  (0, 569)\t0.03058925797651322\n",
      "  (0, 436)\t0.03867132122987755\n",
      "  (0, 416)\t0.01993033174613801\n",
      "  (0, 282)\t0.02602385503526912\n",
      "  (0, 165)\t0.01648300888057191\n",
      "  (0, 97)\t0.07133228576532684\n",
      "  (0, 73)\t0.014863646072189226\n",
      "  (0, 40)\t0.03936432393269186\n",
      "  (0, 18)\t0.018441741863684972\n"
     ]
    }
   ],
   "source": [
    "print(\"email 0:\")\n",
    "print(tfidf[0,:])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Utility function\n",
    "\n",
    "Let's put all this loading process into a separate file so that we can reuse it in other experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import load_spam\n",
    "spam_data = load_spam.spam_data_loader()\n",
    "spam_data.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "email file: ../data/lingspam_public/bare/part6/9-1699msg2.txt\n",
      "email is a spam: False\n",
      "Subject: 7th european nlg workshop\n",
      "\n",
      "7th european workshop on natural language generation call for papers toulouse ( france ) , may 13-15 1999 this seventh european workshop on natural language generation will focus on all aspects of computational linguistics and industrial applications related to natural language generation . papers related to theoretical aspects , applied research and ongoing projects are encouraged . natural language generation being the study of a number of models , approaches and systems , the workshop will include , besides classical topics , scientific domains in which natural language generation plays an important role , such as speech , dialogue , multi-media interfaces , psycho-linguistics , and theoretical linguistics . relevant application domains include all types of applications ( mt , ir and ie , etc . ) in which generation plays an important role and where specific techniques or models of generation have been developed . the main topics are the following , without excluding others : - lexical aspects : phonology , morphology , syntax and semantics , - syntactic aspects , - semantic and pragmatic aspects , - speech synthesis and nlg , oral dialogue , - construction of knowledge bases for nlg , - applications of nlg : mt , summarization , report generation , etc . , - multi-media generation including graphics , numerical information , and texts in various formats , etc . , - psycholinguistic aspects of nlg : in speech production , in discourse production and management , in lexicalization , etc . - architectures for nlg , - internet and web applications using nlg . multiple submissions must be mentioned , and if the paper is accepted at several places , presentation at the workshop will be conditional to the paper been withdrawn from these other places . papers should be prepared in latex ( preferably ) or in word ( send rtf file ) , not exceeding 4000 words ( about 8 to 10 pages long ) , including references . more details about format submissions will be given soon on the workshop web site . papers must relate original , unpublished work . work in progress can also be submitted . papers must include the authors ' name , full address and e-mail . they will be reviewed anonymously . therefore , a title page must come separately , with the title of the paper , the abstract , the authors ' names and addresses , and , if appropriate , the mention of multiple submissions ( and where the paper has been submitted ) . no indication of the authors ' identity must appear in the text of the paper . deadlines : january 25th submission of papers by e-mail march 10th notification of acceptance / rejection april 15th final paper due ( paper copy ) may 13-15 workshop papers and all correspondence should be sent to stdizier @ irit . fr programme committee : christy doran wolfgang hoeppner helmut horacek eduard hovy guy lapalme kathy mccoy david mcdonald kathy mckeown chris mellish cecile paris patrick saint - dizier manfred stede michael zock local organization information : web site ( forthcoming ) : http : / / www . irit . fr / manifs / manifs . html look for : ewnlg ' 99 . the meeting will be held in toulouse downtown , in the holiday inn hotel , on the famous ` place du capitole ' , in the heart of the old town . toulouse has an international airport , with many national and european destinations . there are many places worth visiting in the city , all within about 10 minutes walking distance ( museums , old roman - style churches , 16th - 17th century private houses with inner yards , etc . ) . fees should be around 750 french francs ( about 130 us $ ) , for 2 days , including two lunches , the breaks and the proceedings . for three days , fees will be about 1000f . blocks of rooms will be reserved in the hotel meeting ( 530f per room per day , incl . buffet breakfast ) and in other , cheaper , hotels around at preferential rates .\n",
      "\n",
      "Bag of words representation (154 words in dictionary):\n",
      "{'subject': 1, 'medium': 2, 'sent': 1, 'mail': 2, 'report': 1, 'paper': 7, 'international': 1, 'work': 2, 'private': 1, 'two': 1, 'within': 1, 'day': 3, 'application': 1, 'acceptance': 1, 'number': 1, 'without': 1, 'file': 1, 'information': 2, 'worth': 1, 'copy': 1, 'include': 3, 'appear': 1, 'multiple': 2, 'knowledge': 1, 'place': 1, 'u': 1, 'theoretical': 2, 'applied': 1, 'linguistics': 3, 'call': 1, 'language': 5, 'send': 1, 'page': 1, 'abstract': 1, 'following': 1, 'name': 1, 'address': 1, 'submission': 1, 'web': 3, 'workshop': 7, 'dialogue': 2, 'title': 2, 'research': 1, 'speech': 3, 'computational': 1, 'relevant': 1, 'generation': 9, 'appropriate': 1, 'may': 2, 'progress': 1, 'format': 1, 'oral': 1, 'accepted': 1, 'given': 1, 'come': 1, 'management': 1, 'due': 1, 'scientific': 1, 'committee': 1, 'distance': 1, 'important': 2, 'march': 1, 'notification': 1, 'also': 1, 'site': 2, 'per': 2, 'many': 2, 'several': 1, 'related': 2, 'city': 1, 'full': 1, 'must': 5, 'various': 1, 'classical': 1, 'main': 1, 'semantics': 1, 'natural': 5, 'syntax': 1, 'syntactic': 1, 'semantic': 1, 'focus': 1, 'reserved': 1, 'phonology': 1, 'lexical': 1, 'study': 1, 'three': 1, 'graphic': 1, 'old': 2, 'word': 1, 'final': 1, 'seventh': 1, 'industrial': 1, 'ongoing': 1, 'besides': 1, 'role': 2, 'ie': 1, 'specific': 1, 'excluding': 1, 'morphology': 1, 'pragmatic': 1, 'synthesis': 1, 'construction': 1, 'base': 1, 'summarization': 1, 'numerical': 1, 'production': 2, 'discourse': 1, 'presentation': 1, 'conditional': 1, 'withdrawn': 1, 'prepared': 1, 'latex': 1, 'preferably': 1, 'exceeding': 1, 'long': 1, 'soon': 1, 'relate': 1, 'original': 1, 'unpublished': 1, 'anonymously': 1, 'therefore': 1, 'separately': 1, 'mention': 1, 'indication': 1, 'identity': 1, 'text': 1, 'rejection': 1, 'correspondence': 1, 'guy': 1, 'saint': 1, 'local': 1, 'organization': 1, 'forthcoming': 1, 'look': 1, 'meeting': 2, 'downtown': 1, 'holiday': 1, 'inn': 1, 'hotel': 2, 'famous': 1, 'heart': 1, 'town': 1, 'airport': 1, 'national': 1, 'visiting': 1, 'walking': 1, 'style': 1, 'century': 1, 'inner': 1, 'around': 2, 'room': 1, 'buffet': 1, 'breakfast': 1, 'preferential': 1}\n"
     ]
    }
   ],
   "source": [
    "spam_data.print_email(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
