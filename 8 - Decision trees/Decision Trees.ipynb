{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc-sa/4.0/\"><img alt=\"Creative Commons License\" align=\"left\" src=\"https://i.creativecommons.org/l/by-nc-sa/4.0/80x15.png\" /></a>&nbsp;| [Emmanuel Rachelson](https://personnel.isae-supaero.fr/emmanuel-rachelson?lang=en) | <a href=\"https://supaerodatascience.github.io/machine-learning/\">https://supaerodatascience.github.io/machine-learning/</a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-size:22pt; line-height:25pt; font-weight:bold; text-align:center;\">Decision Trees</div>\n",
    "\n",
    "\n",
    "1. [The Iris dataset](#sec1)\n",
    "2. [What is a decision tree?](#sec2)\n",
    "3. [Growing decision trees](#sec3)\n",
    "4. [A note on the Entropy and the Gini index](#sec4)\n",
    "5. [Generalizing the splitting criterion](#sec5)\n",
    "6. [Practicing with trees](#sec6)\n",
    "7. [Variability of decision trees](#sec7)\n",
    "8. [Conclusion on Decision Trees](#sec8)\n",
    "9. [Examples](#sec9)\n",
    "    1. [Spam or ham?](#sec9-1)\n",
    "    2. [NIST](#sec9-2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id=\"sec1\"></a>1. The Iris dataset\n",
    "- 150 Iris flowers\n",
    "- 3 species (labels)\n",
    "- 4 morphologic features: petal and sepal width and length\n",
    "- 50 examples in each class\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/9/9f/Iris_virginica.jpg/1024px-Iris_virginica.jpg\" width=\"400\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "print(iris.data.shape)\n",
    "print(iris.target.shape)\n",
    "print(iris.target_names)\n",
    "print(iris.feature_names)\n",
    "print(\"\\n\",iris.DESCR)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id=\"sec2\"></a> 2. What is a decision tree?\n",
    "\n",
    "**Hierarchical description of data based on logical (binary) questions**.\n",
    "\n",
    "Ingredients:\n",
    "- Nodes<br>\n",
    "Each node contains a **test** on the features which **partitions** the data.\n",
    "- Edges<br>\n",
    "The outcome of a node's test leads to one of its child edges.\n",
    "- Leaves<br>\n",
    "A terminal node, or leaf, holds a **decision value** for the output variable.\n",
    "<img src=\"attachment:tree1.png\" width=\"400px\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id=\"sec3\"></a>3. Growing decision trees\n",
    "\n",
    "Training Set $T$, attributes $x_1,\\ldots, x_p$\n",
    "\n",
    "<div class= \"alert alert-success\">\n",
    "    \n",
    "**FormTree($T$)**<br>\n",
    "<ol>\n",
    "<li> Find best split $(j, s)$ over $T$ // Which criterion?\n",
    "<li> If $(j, s) = \\emptyset$, \n",
    "    <ul>\n",
    "    <li>  node = FormLeaf(T) // Which value for the leaf?\n",
    "    </ul>\n",
    "<li> Else\n",
    "    <ul>\n",
    "    <li> node = $(j, s)$\n",
    "    <li> split $T$ according to $(j, s)$ into $(T1, T2)$\n",
    "    <li> append FormTree($T1$) to node // Recursive call\n",
    "    <li> append FormTree($T2$) to node\n",
    "    </ul>\n",
    "<li> Return node\n",
    "</ol>\n",
    "</div>\n",
    "\n",
    "**Classification: splitting criterion**\n",
    "\n",
    "Choose split $(j,s)$ that maximizes the entropy (information) gain:\n",
    "$$N_1 \\sum\\limits_{k=1}^K \\hat{p}_{1k} \\log \\hat{p}_{1k} + N_2 \\sum\\limits_{k=1}^K \\hat{p}_{2k} \\log \\hat{p}_{2k}$$\n",
    "with $N_m$: number of examples in node $m$ and $p_{mk}$: proportion of class $k$ in node $m$\n",
    "\n",
    "Alternate criterion: maximize the Gini index gain.\n",
    "$$N_1 \\sum\\limits_{k=1}^K \\hat{p}_{1k} \\left(1-\\hat{p}_{1k}\\right) + N_2 \\sum\\limits_{k=1}^K \\hat{p}_{2k} \\left( 1- \\hat{p}_{2k} \\right)$$\n",
    "\n",
    "**Classification: value of a leaf node**\n",
    "- majority class.\n",
    "\n",
    "**Regression: splitting criterion**\n",
    "\n",
    "We want to minimize the **sum of squares** error in a leaf node:\n",
    "$$\\sum_{i=1}^{N_m} \\left(y_i - \\hat{f}(x_i)\\right)^2$$\n",
    "Where $\\hat{f}(x_i)$ is the prediction in $x_i$. For simplicity, we take constant node-wise predictions $\\hat{f}(x_i)= \\hat{y}_i$. We introduce the node impurity measure as the mean square error:\n",
    "$$Q_m = \\frac{1}{N_m} \\sum_{i=1}^{N_m} \\left(y_i - \\hat{y}_i\\right)^2 $$\n",
    "The best split is then the one that minimizes\n",
    "$$N_1 Q_1 + N_2 Q_2$$\n",
    "\n",
    "**Regression: value of a leaf node**\n",
    "- average\n",
    "\n",
    "Alternatives (but imply to change the splitting criterion):\n",
    "- median\n",
    "- extreme values\n",
    "- B-spline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id=\"sec4\"></a> 4. A note on the Entropy and the Gini index\n",
    "\n",
    "Suppose a set of samples $T=\\left\\{(x,y)\\right\\}$ where $y\\in \\{0;1\\}$ and a constant value for $x$. The labels $y$ give some class information on $x$. One relevant question is \"how much useful information is there in $T$?\".\n",
    "\n",
    "- For example, if half of the examples have label \"0\" and the other half has label \"1\", then there is no real useful information in $T$.\n",
    "- On the other hand, if all the examples have the same label, then $T$ is very informative regarding the real class of $x$.\n",
    "\n",
    "Intuitively, a measure of \"disorder\" in a training set is an opposite measure of \"information\". That's what Shannon theorized in his Information theory.\n",
    "\n",
    "A measure of disorder is the (binary) entropy of a set of bits. Suppose $p$ is the probability of class \"1\":\n",
    "\n",
    "<img src=\"https://wikimedia.org/api/rest_v1/media/math/render/svg/f4a7673eba0df7fb924d538fb6b1c94174f812af\" width=\"200px\">\n",
    "\n",
    "$$H(p) = -p \\log_2(p) - (1-p)\\log_2(1-p)$$\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/c/c9/Binary_entropy_plot.png?20060319144452\" width=\"400px\">\n",
    "\n",
    "When building a tree, we want to choose each split so that it increases the overall information. Hence we want to decrease the overall entropy. Since entropy is additive, if the subscript \"0\" refers of a node and the subscripts \"1\" and \"2\" to its children after a split, we want $N_0 H_0 > N_1 H_1 + N_2 H_2$.\n",
    "\n",
    "The Gini index expresses somehow the same idea. It was introduced by Gini (a sociologist and statistician) in 1936 to describe inequalities of income or wealth within a population. For $K$ classes, it is written:\n",
    "$$\\sum_{k=1}^K p_k(1-p_k)$$\n",
    "\n",
    "To get an intuition of why this is a measure of inequality, suppose a training set $T=\\left\\{(x,y)\\right\\}$ with only two classes $\\{0;1\\}$. Suppose you pick an individuals $x$ at random in $T$ and then you pick a class $y$ at random in $T$ again. What is the probability that $y$ is not the label of $x$?\n",
    "$$p(1-p) + (1-p)(1-(1-p))$$\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id=\"sec5\"></a> 5. Generalizing the splitting criterion\n",
    "\n",
    "Overall it is a matter of decreasing an impurity measure in the current node. This impurity measure is the **fitness** criterion our tree tries to (locally) minimize: entropy, Gini index, mean square error."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id=\"sec6\"></a> 6. Practicing with trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "from os import system\n",
    "from IPython.display import Image\n",
    "\n",
    "iris_dt = tree.DecisionTreeClassifier()\n",
    "iris_dt.fit(iris.data, iris.target)\n",
    "\n",
    "tree.plot_tree(iris_dt)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to limit the depth of the tree to preserve the generalization error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_dt2 = tree.DecisionTreeClassifier(criterion = 'entropy', max_depth=2)\n",
    "iris_dt2.fit(iris.data, iris.target)\n",
    "tree.plot_tree(iris_dt2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try on 2D complex data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def gen_data(seed):\n",
    "    X1, y1 = datasets.make_gaussian_quantiles(cov=2.,\n",
    "                                 n_samples=300, n_features=2,\n",
    "                                 n_classes=2, random_state=seed)\n",
    "    X2, y2 = datasets.make_gaussian_quantiles(mean=(3, 3), cov=1.5,\n",
    "                                 n_samples=700, n_features=2,\n",
    "                                 n_classes=2, random_state=seed)\n",
    "    X = np.concatenate((X1, X2))\n",
    "    y = np.concatenate((y1, - y2 + 1))\n",
    "    y = 2*y-1\n",
    "    X, y = shuffle(X, y)\n",
    "    return X, y\n",
    "\n",
    "X,y = gen_data(1)\n",
    "Xtest,X = np.split(X,[400])\n",
    "ytest,y = np.split(y,[400])\n",
    "\n",
    "Xblue = X[y==-1]\n",
    "Xred = X[y==1]\n",
    "plt.figure(figsize=(7,7))\n",
    "plt.scatter(Xblue[:,0],Xblue[:,1],c='b')\n",
    "_=plt.scatter(Xred[:,0],Xred[:,1],c='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change max_depth parameter to 10\n",
    "dt1 = tree.DecisionTreeClassifier(criterion='entropy',max_depth=10)\n",
    "dt1.fit(X,y)\n",
    "\n",
    "\n",
    "tree.plot_tree(dt1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decision_boundary_tree(t, X, y, fig_size=(7,7)):\n",
    "    plot_step = 0.02\n",
    "    x0_min, x0_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    x1_min, x1_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx0, xx1 = np.meshgrid(np.arange(x0_min, x0_max, plot_step), np.arange(x1_min, x1_max, plot_step))\n",
    "    yypred = t.predict(np.c_[xx0.ravel(),xx1.ravel()])\n",
    "    yypred = yypred.reshape(xx0.shape)\n",
    "    plt.figure(figsize=fig_size)\n",
    "    plt.contourf(xx0, xx1, yypred, cmap=plt.cm.Paired)\n",
    "    y_pred = t.predict(X)\n",
    "    Xblue_good = X[np.equal(y,-1)*np.equal(y,y_pred)]\n",
    "    Xblue_bad  = X[np.equal(y,-1)*np.not_equal(y,y_pred)]\n",
    "    Xred_good  = X[np.equal(y,1)*np.equal(y,y_pred)]\n",
    "    Xred_bad   = X[np.equal(y,1)*np.not_equal(y,y_pred)]\n",
    "    plt.scatter(Xblue_good[:,0],Xblue_good[:,1],c='b')\n",
    "    plt.scatter(Xblue_bad[:,0],Xblue_bad[:,1],c='c',marker='v')\n",
    "    plt.scatter(Xred_good[:,0],Xred_good[:,1],c='r')\n",
    "    plt.scatter(Xred_bad[:,0],Xred_bad[:,1],c='m',marker='v')\n",
    "    plt.show()\n",
    "\n",
    "plot_decision_boundary_tree(dt1, X, y)\n",
    "print(\"Training error: %g\"%(1-dt1.score(X,y)))\n",
    "print(\"Generalization error: %g\"%(1-dt1.score(Xtest,ytest)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id=\"sec7\"></a> 7. Variability of decision trees\n",
    "Let's generate different data sets from the same distribution and compare how different the classification trees are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_size = (7,7)\n",
    "\n",
    "# example 1\n",
    "X,y = gen_data(1)\n",
    "dt1 = tree.DecisionTreeClassifier(criterion='entropy',max_depth=5)\n",
    "dt1.fit(X,y)\n",
    "plot_decision_boundary_tree(dt1, X, y, fig_size)\n",
    "\n",
    "# example 2\n",
    "X,y = gen_data(3)\n",
    "dt1 = tree.DecisionTreeClassifier(criterion='entropy',max_depth=5)\n",
    "dt1.fit(X,y)\n",
    "plot_decision_boundary_tree(dt1, X, y, fig_size)\n",
    "\n",
    "# example 3\n",
    "X,y = gen_data(5)\n",
    "dt1 = tree.DecisionTreeClassifier(criterion='entropy',max_depth=5)\n",
    "dt1.fit(X,y)\n",
    "plot_decision_boundary_tree(dt1, X, y, fig_size)\n",
    "\n",
    "# example 4\n",
    "X,y = gen_data(9)\n",
    "dt1 = tree.DecisionTreeClassifier(criterion='entropy',max_depth=5)\n",
    "dt1.fit(X,y)\n",
    "plot_decision_boundary_tree(dt1, X, y, fig_size)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id=\"sec8\"></a> 8. Conclusion on Decision Trees\n",
    "\n",
    "## Summary:\n",
    "\n",
    "Advantages of decision trees:\n",
    "- Simple to read and to interpret, easy to visualize.\n",
    "- Learning the tree has complexity linear in the number of data points.\n",
    "- Forward pass has complexity logarithmic in the number of data points used for training.\n",
    "\n",
    "Drawbacks of decision trees:\n",
    "- Very limited representation power (piecewise constant model).\n",
    "- Very sensitive to overfitting, bad generalization.\n",
    "- **Very dependent on the input data.**\n",
    "- No margin or performance guarantees.\n",
    "   \n",
    "- Often used with ensemble methods (Boosting, Bagging).\n",
    "\n",
    "## Further:\n",
    "- How to prune decision trees?\n",
    "- How to handle missing values?\n",
    "- What about preprocessing data?\n",
    "- What comparison can we make between PCA and trees?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id=\"sec9\"></a> 9. Examples\n",
    "\n",
    "## <a id=\"sec9-1\"></a>9.1 Spam or ham?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sys import path\n",
    "path.append('../2 - Text data preprocessing')\n",
    "import load_spam\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('words')\n",
    "nltk.download('wordnet')\n",
    "spam_data = load_spam.spam_data_loader()\n",
    "spam_data.load_data()\n",
    "\n",
    "Xtrain, ytrain, Xtest, ytest = spam_data.split(2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/code1.py\n",
    "### WRITE YOUR CODE HERE\n",
    "# If you get stuck, uncomment the line above to load a correction in this cell (then you can execute this code).\n",
    "\n",
    "spam_dt = tree.DecisionTreeClassifier(criterion='entropy', max_depth=20, min_samples_leaf=10)\n",
    "spam_dt.fit(Xtrain,ytrain)\n",
    "\n",
    "print(\"score:\", spam_dt.score(Xtest,ytest))\n",
    "disp_tree('spam_dt',spam_dt)\n",
    "\n",
    "# Compute cross-validation score\n",
    "nb_trials = 30\n",
    "score = []\n",
    "for i in range(nb_trials):\n",
    "    Xtrain, ytrain, Xtest, ytest = spam_data.shuffle_and_split(2000)\n",
    "    spam_dt = tree.DecisionTreeClassifier(criterion='entropy', max_depth=20, min_samples_leaf=10)\n",
    "    spam_dt.fit(Xtrain,ytrain)\n",
    "    score += [spam_dt.score(Xtest,ytest)]\n",
    "    print('*', end='')\n",
    "print(\" done!\")\n",
    "\n",
    "print(\"Average generalization score:\", np.mean(score))\n",
    "print(\"Standard deviation:\", np.std(score))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look for the best values for `max_depth` and `min_samples_leaf`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depths = np.arange(10,31,2)\n",
    "leaf_size = np.arange(3,16,2)\n",
    "nb_trials = 10\n",
    "scores = np.zeros((depths.shape[0],leaf_size.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for di in range(depths.shape[0]):\n",
    "    for szi in range(leaf_size.shape[0]):\n",
    "        score = []\n",
    "        for i in range(nb_trials):\n",
    "            Xtrain, ytrain, Xtest, ytest = spam_data.shuffle_and_split(2000)\n",
    "            spam_dt = tree.DecisionTreeClassifier(criterion='entropy',\n",
    "                                                  max_depth=depths[di],\n",
    "                                                  min_samples_leaf=leaf_size[szi])\n",
    "            spam_dt.fit(Xtrain,ytrain)\n",
    "            score += [spam_dt.score(Xtest,ytest)]\n",
    "        print('*', end='')\n",
    "        scores[di,szi] = np.mean(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "import numpy as np\n",
    "\n",
    "X, Y = np.meshgrid(leaf_size,depths)\n",
    "\n",
    "%matplotlib inline\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "ax = fig.gca(projection='3d')\n",
    "surf = ax.plot_surface(X, Y, scores, cmap=cm.coolwarm,\n",
    "                       linewidth=0, antialiased=False)\n",
    "\n",
    "ind = np.unravel_index(np.argmax(scores, axis=None), scores.shape)\n",
    "print(\"maximum score of\", scores[ind], \"obtained at max_depth =\", depths[ind[0]], \"and min_samples_leaf =\", leaf_size[ind[1]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's evaluate the training on raw word counts, rather than Tf-Idf features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain, ytrain, Xtest, ytest = spam_data.split(2000, feat='wordcount')\n",
    "# Compute cross-validation score\n",
    "nb_trials = 30\n",
    "score = []\n",
    "for i in range(nb_trials):\n",
    "    Xtrain, ytrain, Xtest, ytest = spam_data.shuffle_and_split(2000)\n",
    "    spam_dt = tree.DecisionTreeClassifier(criterion='entropy', max_depth=20, min_samples_leaf=10)\n",
    "    spam_dt.fit(Xtrain,ytrain)\n",
    "    score += [spam_dt.score(Xtest,ytest)]\n",
    "    print('*', end='')\n",
    "print(\" done!\")\n",
    "\n",
    "print(\"Average generalization score:\", np.mean(score))\n",
    "print(\"Standard deviation:\", np.std(score))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"sec9-2\"></a> 9.2. NIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "digits = datasets.load_digits()\n",
    "#print(digits.data.shape)\n",
    "#print(digits.images.shape)\n",
    "#print(digits.target.shape)\n",
    "#print(digits.target_names)\n",
    "\n",
    "X = digits.data\n",
    "y = digits.target\n",
    "Xtrain,Xtest = np.split(X,[1000])\n",
    "ytrain,ytest = np.split(y,[1000])\n",
    "\n",
    "#print(digits.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "def shuffle_and_split(X,y,n):\n",
    "    X0,y0 = shuffle(X,y)\n",
    "    Xtrain,Xtest = np.split(X0,[n])\n",
    "    ytrain,ytest = np.split(y0,[n])\n",
    "    return Xtrain, ytrain, Xtest, ytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Xtrain.shape)\n",
    "print(ytrain.shape)\n",
    "digits_dt = tree.DecisionTreeClassifier(criterion='entropy', max_depth=20, min_samples_leaf=10)\n",
    "digits_dt.fit(Xtrain,ytrain)\n",
    "prediction = digits_dt.predict(Xtest)\n",
    "#print(\"Training error:\", np.sum(np.not_equal(prediction,ytest))/len(ytest))\n",
    "print(\"Generalization error:\", np.sum(np.not_equal(prediction,ytest))/len(ytest) )\n",
    "print(\"Generalization score:\", digits_dt.score(Xtest,ytest))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id=\"sec9\"></a> 9.3 Penguins Examples\n",
    "The palmerpenguins data contains size measurements for three penguin species observed on three islands in the Palmer Archipelago, Antarctica.\n",
    "These data were collected from 2007 - 2009 by Dr. Kristen Gorman with the Palmer Station Long Term Ecological Research Program, part of the US Long Term Ecological Research Network. \n",
    "\n",
    "[Download the data](https://www.kaggle.com/parulpandey/palmer-archipelago-antarctica-penguin-data)\n",
    "\n",
    "**Questions**:\n",
    "- Using a scatterplot, highlight the differences between the species on two relevant axis\n",
    "- Using a Decision Tree classification, explain in more details what differentiate the 3 species"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.read_csv(\"../data/penguins/penguins_size.csv\",encoding = \"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "281px",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
