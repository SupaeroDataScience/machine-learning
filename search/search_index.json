{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"index.html","text":"Supervised and Unsupervised Machine Learning \ud83d\udd17 This is the Supervised and Unsupervised Learning section of the Algorithms in Machine Learning class at ISAE-Supaero. The Neural Networks and Deep Learning topics have a class of their own and won't be covered here. Home Github repository Syllabus \ud83d\udd17 This course offers a discovery of the landscape of Machine Learning through some key algorithms. It follows the Statistical Foundations of Machine Learning part and will be followed by the Neural Networks and Deep Learning part . Over 25 hours, we will cover the landscape of Supervised and Unsupervised Machine Learning by taking four different points of view and formalisms: the geometrical perspective, the probabilistic (Bayesian) perspective, the connectionnist perspective (which we will postpone to the next class on Deep Learning), and the ensemble perspective. The class is organized in 9 sessions: Introduction. Introduction to the landscape and workflow of Machine Learning, a few words on specificities of Unsupervised Learning, the importance of data preprocessing (1 session). A geometrical approach to Machine Learning. Support Vector Machines, the bias/variance tradeoff and a bit of kernel theory. A Bayesian perspective on Machine Learning. Naive Bayes Classification and Gaussian Processes, followed by an in-depth class on Surrogate Modeling and Bayesian Optimization. Committee learning methods. Decision Trees and Boosting, followed by an in-depth class on Gradient Boosting. Bagging and Random Forests, followed by an in-depth class on anomaly detection. A \"perspectives\" final session. Explainability in Machine Learning. The pedagogy taken mixes voluntarily hands-on practice in Python with theoretical and mathematical understanding of the methods. At the end of the course you will be able to make an informed choice between the main families of ML algorithms, depending on the problem at hand. You will have an understanding of the algorithmic and mathematical properties of each family of methods and you will have a basic practical knowledge of the Scikit-Learn library. Bibliography \ud83d\udd17 The Elements of Statistical Learning. T. Hastie, R. Tibshirani, J. Friedman. Springer Series in Statistics. https://web.stanford.edu/~hastie/ElemStatLearn/","title":"Home"},{"location":"index.html#supervised-and-unsupervised-machine-learning","text":"This is the Supervised and Unsupervised Learning section of the Algorithms in Machine Learning class at ISAE-Supaero. The Neural Networks and Deep Learning topics have a class of their own and won't be covered here. Home Github repository","title":"Supervised and Unsupervised Machine Learning"},{"location":"index.html#syllabus","text":"This course offers a discovery of the landscape of Machine Learning through some key algorithms. It follows the Statistical Foundations of Machine Learning part and will be followed by the Neural Networks and Deep Learning part . Over 25 hours, we will cover the landscape of Supervised and Unsupervised Machine Learning by taking four different points of view and formalisms: the geometrical perspective, the probabilistic (Bayesian) perspective, the connectionnist perspective (which we will postpone to the next class on Deep Learning), and the ensemble perspective. The class is organized in 9 sessions: Introduction. Introduction to the landscape and workflow of Machine Learning, a few words on specificities of Unsupervised Learning, the importance of data preprocessing (1 session). A geometrical approach to Machine Learning. Support Vector Machines, the bias/variance tradeoff and a bit of kernel theory. A Bayesian perspective on Machine Learning. Naive Bayes Classification and Gaussian Processes, followed by an in-depth class on Surrogate Modeling and Bayesian Optimization. Committee learning methods. Decision Trees and Boosting, followed by an in-depth class on Gradient Boosting. Bagging and Random Forests, followed by an in-depth class on anomaly detection. A \"perspectives\" final session. Explainability in Machine Learning. The pedagogy taken mixes voluntarily hands-on practice in Python with theoretical and mathematical understanding of the methods. At the end of the course you will be able to make an informed choice between the main families of ML algorithms, depending on the problem at hand. You will have an understanding of the algorithmic and mathematical properties of each family of methods and you will have a basic practical knowledge of the Scikit-Learn library.","title":"Syllabus"},{"location":"index.html#bibliography","text":"The Elements of Statistical Learning. T. Hastie, R. Tibshirani, J. Friedman. Springer Series in Statistics. https://web.stanford.edu/~hastie/ElemStatLearn/","title":"Bibliography"},{"location":"anomaly.html","text":"Anomaly detection \ud83d\udd17 Home Github repository This class introduces the problem framing and methodology of Anomaly Detection. It illustrates why classical supervised ML algorithms are not suitable for such problems, and provides new approaches with outlier detection and novelty detection. You will discover, by alternating theory and practice exercises, the major algorithms, principles and warning signs for such tasks, including One-Class SVMs, Local Outlier Factor or Isolation Forest. You will also discover semi-supervised approaches where the error of supervised learning models can turn into anomaly scores. At the end, a practical use case with anonymized aircraft sensor data is proposed, where you will have to develop the whole methodology without guidance. It will help you reflect on the main stakes and warning points of such tasks, to prepare you to address customers in your professional life. Lecture notes Notebook for class exercises ( colab ) Solutions ( colab ) Practical use case instructions Data for use case Requirements for local installation \ud83d\udd17 To setup the Anaconda environment with required dependencies, execute the following instructions in Anaconda prompt or Linux shell. # Clone this github repository on your machine git clone https://github.com/jfabrice/ml-class-anomaly-detection.git # Change working directory inside the repo cd ml-class-anomaly-detection # Create a new virtual environment conda create -n anomalydetectionenv python == 3 .6 # Activate the environment ## For Linux / MAC source activate anomalydetectionenv ## For Windows activate anomalydetectionenv # Install the requirements pip install -r requirements.txt References \ud83d\udd17 A Fast Algorithm for the Minimum Covariance Determinant Estimator. P. J. Rousseeuw, and K. V. Driessen. Technometrics, 41 (3), 212-223, (1999). Estimating the support of a high-dimensional distribution. B. Sch\u00f6lkopf, J. C. Platt, J. Shawe-Taylor, A. J. Smola, and R. C. Williamson. Neural computation, 13 (7), 1443-1471, (2001). Isolation-based anomaly detection. F. T. Liu, K. M. Ting, and Z. H. Zhou. ACM Transactions on Knowledge Discovery from Data, 6 (1), 1-39, (2012). LOF: identifying density-based local outliers. M. M. Breunig, H. P. Kriegel, R. T. Ng, and J. Sander. In ACM SIGMOD International Conference on Management of Data, 93-104, (2000).","title":"Anomaly Detection"},{"location":"anomaly.html#anomaly-detection","text":"Home Github repository This class introduces the problem framing and methodology of Anomaly Detection. It illustrates why classical supervised ML algorithms are not suitable for such problems, and provides new approaches with outlier detection and novelty detection. You will discover, by alternating theory and practice exercises, the major algorithms, principles and warning signs for such tasks, including One-Class SVMs, Local Outlier Factor or Isolation Forest. You will also discover semi-supervised approaches where the error of supervised learning models can turn into anomaly scores. At the end, a practical use case with anonymized aircraft sensor data is proposed, where you will have to develop the whole methodology without guidance. It will help you reflect on the main stakes and warning points of such tasks, to prepare you to address customers in your professional life. Lecture notes Notebook for class exercises ( colab ) Solutions ( colab ) Practical use case instructions Data for use case","title":"Anomaly detection"},{"location":"anomaly.html#requirements-for-local-installation","text":"To setup the Anaconda environment with required dependencies, execute the following instructions in Anaconda prompt or Linux shell. # Clone this github repository on your machine git clone https://github.com/jfabrice/ml-class-anomaly-detection.git # Change working directory inside the repo cd ml-class-anomaly-detection # Create a new virtual environment conda create -n anomalydetectionenv python == 3 .6 # Activate the environment ## For Linux / MAC source activate anomalydetectionenv ## For Windows activate anomalydetectionenv # Install the requirements pip install -r requirements.txt","title":"Requirements for local installation"},{"location":"anomaly.html#references","text":"A Fast Algorithm for the Minimum Covariance Determinant Estimator. P. J. Rousseeuw, and K. V. Driessen. Technometrics, 41 (3), 212-223, (1999). Estimating the support of a high-dimensional distribution. B. Sch\u00f6lkopf, J. C. Platt, J. Shawe-Taylor, A. J. Smola, and R. C. Williamson. Neural computation, 13 (7), 1443-1471, (2001). Isolation-based anomaly detection. F. T. Liu, K. M. Ting, and Z. H. Zhou. ACM Transactions on Knowledge Discovery from Data, 6 (1), 1-39, (2012). LOF: identifying density-based local outliers. M. M. Breunig, H. P. Kriegel, R. T. Ng, and J. Sander. In ACM SIGMOD International Conference on Management of Data, 93-104, (2000).","title":"References"},{"location":"bagging.html","text":"Bagging \ud83d\udd17 Home Github repository In this class, we will introduce the bootstrap method and its application to learning a predictor called Bagging (Bootstrap AGGregatING). First we review bootstrap in statistics as a method to estimate the variance of an estimator on any statistic of a random variable (e.g. its mean). Then we extend this notion to machine learning, i.e., to learning a predictor for regression or classification. We discuss the pros and cons of bagging. Notebook References \ud83d\udd17 An Introduction to the Bootstrap. B. Efron and R. Tibshirani, Chapman & Hall/CRC, (1993). What Teachers Should Know About the Bootstrap: Resampling in the Undergraduate Statistics Curriculum. T. C. Hesterberg, The American Statistician, 69(4), 371\u2013386, (2015).","title":"Bagging"},{"location":"bagging.html#bagging","text":"Home Github repository In this class, we will introduce the bootstrap method and its application to learning a predictor called Bagging (Bootstrap AGGregatING). First we review bootstrap in statistics as a method to estimate the variance of an estimator on any statistic of a random variable (e.g. its mean). Then we extend this notion to machine learning, i.e., to learning a predictor for regression or classification. We discuss the pros and cons of bagging. Notebook","title":"Bagging"},{"location":"bagging.html#references","text":"An Introduction to the Bootstrap. B. Efron and R. Tibshirani, Chapman & Hall/CRC, (1993). What Teachers Should Know About the Bootstrap: Resampling in the Undergraduate Statistics Curriculum. T. C. Hesterberg, The American Statistician, 69(4), 371\u2013386, (2015).","title":"References"},{"location":"boost.html","text":"Boosting \ud83d\udd17 Home Github repository In this class, we introduce the principle of boosting, which can be seen as an incremental way to build a \"strong\" classifier with \"weak\" classifiers. As is, this technic is an ensemble method. More specifically, the \"weak\" classifiers are added sequentially, so that the new model compensates the flaws of the ensemble composed of the previous models. Further, we introduce the gradient boosting boosting as a generalization of boosting but using gradients for the incremental addition of models. Notebook References \ud83d\udd17 The Boosting Approach to Machine Learning An Overview. R. E. Schapire. MSRI workshop on Nonlinear Estimation and Classification, (2002).","title":"Boosting"},{"location":"boost.html#boosting","text":"Home Github repository In this class, we introduce the principle of boosting, which can be seen as an incremental way to build a \"strong\" classifier with \"weak\" classifiers. As is, this technic is an ensemble method. More specifically, the \"weak\" classifiers are added sequentially, so that the new model compensates the flaws of the ensemble composed of the previous models. Further, we introduce the gradient boosting boosting as a generalization of boosting but using gradients for the incremental addition of models. Notebook","title":"Boosting"},{"location":"boost.html#references","text":"The Boosting Approach to Machine Learning An Overview. R. E. Schapire. MSRI workshop on Nonlinear Estimation and Classification, (2002).","title":"References"},{"location":"gp.html","text":"Gaussian Processes \ud83d\udd17 Home Github repository This class continues our exploration of the Bayesian approach to Machine Learning. Departing from the question of estimating $P(y|x)$ and the hypothesis that a set of observations y is distributed as a Gaussian vector around its meand, we shall see how we can derive an explicit form for the distribution of $y|x$. This explicit form will be called a Gaussian Process and will provide us with the likelihood that each possible function fits our data. In turn, this will provide an elegant and efficient way to estimate the most likely output $y$ for a given $x$ but also the confidence interval around this prediction. Notebook ( colab ) Pre-class refresher activities and solution Summary card References \ud83d\udd17 Gaussian Processes in Machine Learning. C. E. Rasmussen and C. K. I. Williams. MIT press , 2005. Available for download at http://www.gaussianprocess.org/gpml.","title":"Gaussian Processes"},{"location":"gp.html#gaussian-processes","text":"Home Github repository This class continues our exploration of the Bayesian approach to Machine Learning. Departing from the question of estimating $P(y|x)$ and the hypothesis that a set of observations y is distributed as a Gaussian vector around its meand, we shall see how we can derive an explicit form for the distribution of $y|x$. This explicit form will be called a Gaussian Process and will provide us with the likelihood that each possible function fits our data. In turn, this will provide an elegant and efficient way to estimate the most likely output $y$ for a given $x$ but also the confidence interval around this prediction. Notebook ( colab ) Pre-class refresher activities and solution Summary card","title":"Gaussian Processes"},{"location":"gp.html#references","text":"Gaussian Processes in Machine Learning. C. E. Rasmussen and C. K. I. Williams. MIT press , 2005. Available for download at http://www.gaussianprocess.org/gpml.","title":"References"},{"location":"intro.html","text":"Introduction to Machine Learning \ud83d\udd17 Home Github repository This class provides an introduction to the landscape, zoo, and workflow of Machine Learning. Slides","title":"Introduction to ML"},{"location":"intro.html#introduction-to-machine-learning","text":"Home Github repository This class provides an introduction to the landscape, zoo, and workflow of Machine Learning. Slides","title":"Introduction to Machine Learning"},{"location":"mlc.html","text":"An application of SVMs to multi-label classification \ud83d\udd17 Home Github repository This short application session proposes to use SVMs as a building block for Multi-Label Classification. It serves both as a practice session for the previous class and as an introduction to Multi-Label Classification. Notebook source Notebook on Colab References \ud83d\udd17 J. Read, P. Reutemann, B. Pfahringer, and Geoff Holmes. MEKA: A multi-label/multi-target extension to Weka . Journal of Machine Learning Research, 17(21):1\u20135, 2016. J. Read, B. Pfahringer, G. Holmes, and E. Frank. Classifier chains for multi-label classification . Joint European Conference on Machine Learning and Knowledge Discovery in Databases, pages 254\u2013269, 2009. G. Tsoumakas and I. Katakis. Multi-label classification: An overview . International Journal on Data Warehousing and Mining, 3(3):1\u201313, 2007. G. Tsoumakas, I. Katakis, and I. Vlahavas. Mining multi-label data . Data mining and knowledge discovery handbook, pages 667\u2013685. Springer, 2010. G. Tsoumakas, I. Katakis, and I. Vlahavas. Random k-labelsets for multi-label classification . IEEE Transactions on Knowledge and Data Engineering, 23(7):1079-1089, 2011.","title":"An application of SVMs to multi-label classification"},{"location":"mlc.html#an-application-of-svms-to-multi-label-classification","text":"Home Github repository This short application session proposes to use SVMs as a building block for Multi-Label Classification. It serves both as a practice session for the previous class and as an introduction to Multi-Label Classification. Notebook source Notebook on Colab","title":"An application of SVMs to multi-label classification"},{"location":"mlc.html#references","text":"J. Read, P. Reutemann, B. Pfahringer, and Geoff Holmes. MEKA: A multi-label/multi-target extension to Weka . Journal of Machine Learning Research, 17(21):1\u20135, 2016. J. Read, B. Pfahringer, G. Holmes, and E. Frank. Classifier chains for multi-label classification . Joint European Conference on Machine Learning and Knowledge Discovery in Databases, pages 254\u2013269, 2009. G. Tsoumakas and I. Katakis. Multi-label classification: An overview . International Journal on Data Warehousing and Mining, 3(3):1\u201313, 2007. G. Tsoumakas, I. Katakis, and I. Vlahavas. Mining multi-label data . Data mining and knowledge discovery handbook, pages 667\u2013685. Springer, 2010. G. Tsoumakas, I. Katakis, and I. Vlahavas. Random k-labelsets for multi-label classification . IEEE Transactions on Knowledge and Data Engineering, 23(7):1079-1089, 2011.","title":"References"},{"location":"nbc.html","text":"Naive Bayes Classification \ud83d\udd17 Home Github repository This class provides a gentle introduction to the Bayesian approach to Supervised Learning. We will consider the question of estimating the distribution of a label $y$ given an input $x$ and some data. A first, very naive way to tackle this problem will involve a strong hypothesis of conditional independence, which we will call the naive Bayes assumption. This will open the door to the method of Naive Bayes Classification. Notebook ( colab ) Pre-class refresher activities and solution Summary card Lecture notes References \ud83d\udd17 Exploring conditions for the optimality of naive Bayes. Zhang, H. International Journal of Pattern Recognition and Artificial Intelligence , 19 (02), 183-198, (2005).","title":"Naive Bayes Classification"},{"location":"nbc.html#naive-bayes-classification","text":"Home Github repository This class provides a gentle introduction to the Bayesian approach to Supervised Learning. We will consider the question of estimating the distribution of a label $y$ given an input $x$ and some data. A first, very naive way to tackle this problem will involve a strong hypothesis of conditional independence, which we will call the naive Bayes assumption. This will open the door to the method of Naive Bayes Classification. Notebook ( colab ) Pre-class refresher activities and solution Summary card Lecture notes","title":"Naive Bayes Classification"},{"location":"nbc.html#references","text":"Exploring conditions for the optimality of naive Bayes. Zhang, H. International Journal of Pattern Recognition and Artificial Intelligence , 19 (02), 183-198, (2005).","title":"References"},{"location":"nn.html","text":"Neural networks \ud83d\udd17 This part of the class grew and now has its own full course. It is the upcoming Neural Networks and Deep Learning class.","title":"Neural networks"},{"location":"nn.html#neural-networks","text":"This part of the class grew and now has its own full course. It is the upcoming Neural Networks and Deep Learning class.","title":"Neural networks"},{"location":"preproc.html","text":"The importance of data pre-processing \ud83d\udd17 Home Github repository Through an example of text data preparation, this class illustrates why it is crucial to think about data preprocessing before starting thinking about algorithms in ML. Notebook ( colab )","title":"The importance of data preprocessing"},{"location":"preproc.html#the-importance-of-data-pre-processing","text":"Home Github repository Through an example of text data preparation, this class illustrates why it is crucial to think about data preprocessing before starting thinking about algorithms in ML. Notebook ( colab )","title":"The importance of data pre-processing"},{"location":"rf.html","text":"Random Forests \ud83d\udd17 Home Github repository In this class, we will introduce Random Forest as a new boosting machine algorithm using randomness in two ways to incrementally add trees: By sub-sampling a random training set in the original training set as in bagging methods. By selecting a random subset of features on which performing tree splits for each choice of split. The method is then showcased in simple classification tasks. Notebook References \ud83d\udd17 Wikipedia page on Random Forest Random Forests. Breiman, Leo, Machine learning 45.1 (2001): 5-32. Understanding random forests: From theory to practice. Louppe, Gilles, PhD thesis.","title":"Random Forests"},{"location":"rf.html#random-forests","text":"Home Github repository In this class, we will introduce Random Forest as a new boosting machine algorithm using randomness in two ways to incrementally add trees: By sub-sampling a random training set in the original training set as in bagging methods. By selecting a random subset of features on which performing tree splits for each choice of split. The method is then showcased in simple classification tasks. Notebook","title":"Random Forests"},{"location":"rf.html#references","text":"Wikipedia page on Random Forest Random Forests. Breiman, Leo, Machine learning 45.1 (2001): 5-32. Understanding random forests: From theory to practice. Louppe, Gilles, PhD thesis.","title":"References"},{"location":"surrog.html","text":"Surrogate Modeling and Bayesian Optimization \ud83d\udd17 Home Github repository This practical session explores the use of Gaussian Processes in order to estimate a given physics phenomenon, say forces and torques on an aircraft's wing. Such a model is called a surrogate model of the actual phenomenon. This surrogate model is then used to optimize the mechanical characteristics of the airfoil, without resorting to costly numerical simulations or experiments. We explore methods for Bayesian optimization using surrogate models in the last part of the class. Notebook ( colab )","title":"Surrogate Modeling and Bayesian Optimization"},{"location":"surrog.html#surrogate-modeling-and-bayesian-optimization","text":"Home Github repository This practical session explores the use of Gaussian Processes in order to estimate a given physics phenomenon, say forces and torques on an aircraft's wing. Such a model is called a surrogate model of the actual phenomenon. This surrogate model is then used to optimize the mechanical characteristics of the airfoil, without resorting to costly numerical simulations or experiments. We explore methods for Bayesian optimization using surrogate models in the last part of the class. Notebook ( colab )","title":"Surrogate Modeling and Bayesian Optimization"},{"location":"svm.html","text":"Support Vector Machines, the bias-variance tradeoff and a bit of kernel theory \ud83d\udd17 Home Github repository This class takes a geometrical approach to Machine Learning through the prism of Support Vector Machines. It covers linear classifiers for data separation first. On the way, it introduces the bias/variance tradeoff. Then it presents a bit of kernel theory and applies it in linear classifiers to reach non-linear SVMs. It then provides perspectives on multi-class classification, support vector regression and density estimation with one-class SVM. Two full practical examples are provided at the end. Notebook ( colab ) Pre-class refresher activities and solution Summary card Lecture notes References \ud83d\udd17 On the general theory of SVMs for classification: A tutorial on Support Vector Machines for Pattern Recognition. C. J. C. Burges, Data Mining and Knowledge Discovery , 2 , 131-167, (1998). On support vector regression (and its extension to $\\nu$-SVR): A tutorial on Support Vector Regression. A. J. Smola and B. Sch\u00f6lkopf, Journal of Statistics and Computing , 14 (3), 199-222, (2004). New support vector algorithms. B. Sch\u00f6lkopf, A. J. Smola, R. C. Williamson, and P. L. Bartlett. Neural computation , 12 (5), 1207-1245, (2000). On One-Class SVMs: Support vector method for novelty detection. B. Sch\u00f6lkopf, R. C. Williamson, A. J. Smola, J. Shawe-Taylor, and John C. Platt. Neural Information Processing Systems , 12 , 582-588, (1999). On multi-class SVMs: On the algorithmic implementation of multiclass kernel-based vector machines. K. Crammer and Y. Singer. Journal of machine learning research , 2 , 265-292, (2001).","title":"SVMs, the bias-variance tradeoff and a bit of kernel theory"},{"location":"svm.html#support-vector-machines-the-bias-variance-tradeoff-and-a-bit-of-kernel-theory","text":"Home Github repository This class takes a geometrical approach to Machine Learning through the prism of Support Vector Machines. It covers linear classifiers for data separation first. On the way, it introduces the bias/variance tradeoff. Then it presents a bit of kernel theory and applies it in linear classifiers to reach non-linear SVMs. It then provides perspectives on multi-class classification, support vector regression and density estimation with one-class SVM. Two full practical examples are provided at the end. Notebook ( colab ) Pre-class refresher activities and solution Summary card Lecture notes","title":"Support Vector Machines, the bias-variance tradeoff and a bit of kernel theory"},{"location":"svm.html#references","text":"On the general theory of SVMs for classification: A tutorial on Support Vector Machines for Pattern Recognition. C. J. C. Burges, Data Mining and Knowledge Discovery , 2 , 131-167, (1998). On support vector regression (and its extension to $\\nu$-SVR): A tutorial on Support Vector Regression. A. J. Smola and B. Sch\u00f6lkopf, Journal of Statistics and Computing , 14 (3), 199-222, (2004). New support vector algorithms. B. Sch\u00f6lkopf, A. J. Smola, R. C. Williamson, and P. L. Bartlett. Neural computation , 12 (5), 1207-1245, (2000). On One-Class SVMs: Support vector method for novelty detection. B. Sch\u00f6lkopf, R. C. Williamson, A. J. Smola, J. Shawe-Taylor, and John C. Platt. Neural Information Processing Systems , 12 , 582-588, (1999). On multi-class SVMs: On the algorithmic implementation of multiclass kernel-based vector machines. K. Crammer and Y. Singer. Journal of machine learning research , 2 , 265-292, (2001).","title":"References"},{"location":"trees.html","text":"Decision trees \ud83d\udd17 Home Github repository This class introduces the concept of decision trees as supervized learning methods for classification and regression. A decision tree is a simple model that can even be visualized and understood by a human after training. It consists in a graph with a \"tree\" structure, meaning that there exists a root node with a pair of child nodes, themself having pairs of child nodes, etc., until some leaf nodes. The way data are processed by the model is by flowing through the nodes until reaching a leaf, that corresponds to a class or a value. At each node, a \"split\" was created at training time, testing some features of the data point. The binary outcome of the test determines whether the next node seen by the data will be the first or the second child of the current node. Notebook References \ud83d\udd17 Scikit-learn documentation on decision trees Wikipedia on decision tree learning T. Hastie, R. Tibshirani and J. Friedman. Elements of Statistical Learning, Springer, 2009.","title":"Decision trees"},{"location":"trees.html#decision-trees","text":"Home Github repository This class introduces the concept of decision trees as supervized learning methods for classification and regression. A decision tree is a simple model that can even be visualized and understood by a human after training. It consists in a graph with a \"tree\" structure, meaning that there exists a root node with a pair of child nodes, themself having pairs of child nodes, etc., until some leaf nodes. The way data are processed by the model is by flowing through the nodes until reaching a leaf, that corresponds to a class or a value. At each node, a \"split\" was created at training time, testing some features of the data point. The binary outcome of the test determines whether the next node seen by the data will be the first or the second child of the current node. Notebook","title":"Decision trees"},{"location":"trees.html#references","text":"Scikit-learn documentation on decision trees Wikipedia on decision tree learning T. Hastie, R. Tibshirani and J. Friedman. Elements of Statistical Learning, Springer, 2009.","title":"References"},{"location":"unsup.html","text":"A quick tour of Unsupervised Learning \ud83d\udd17 Home Github repository This class links some important concepts seen in the previous classes of Statistics and the Introduction to Statistical Learning, with the concept of Unsupervised Learning and its specificities. Notebook ( colab )","title":"A quick tour of Unsupervised Learning"},{"location":"unsup.html#a-quick-tour-of-unsupervised-learning","text":"Home Github repository This class links some important concepts seen in the previous classes of Statistics and the Introduction to Statistical Learning, with the concept of Unsupervised Learning and its specificities. Notebook ( colab )","title":"A quick tour of Unsupervised Learning"},{"location":"xai.html","text":"Explainability in ML \ud83d\udd17","title":"Explainability in ML"},{"location":"xai.html#explainability-in-ml","text":"","title":"Explainability in ML"},{"location":"xgboost.html","text":"Gradient Boosting and XGBoost \ud83d\udd17 Home Github repository In this class, you will learn to use the XGBoost library, which efficiently implements gradient boosting algorithms. This Practice Course is composed of 3 parts - each part is meant to be done in about 1 hour : * In the first notebook , you will learn the basic of XGBoost , how to apply it on a dataset and tune it to obtain the best performances. * In the second notebook , we will focus on ensemble methods and explain what makes XGBoost different from other models. * Finally in the last notebook you will see how the choice of hyperparameters is a key element of a tradeoff between Bias and Variance . Notebook 1: Introduction to XGBoost Notebook 2: XGBoost and ensemble models Notebook 3: Regularization References \ud83d\udd17 XGBoost","title":"XGBoost"},{"location":"xgboost.html#gradient-boosting-and-xgboost","text":"Home Github repository In this class, you will learn to use the XGBoost library, which efficiently implements gradient boosting algorithms. This Practice Course is composed of 3 parts - each part is meant to be done in about 1 hour : * In the first notebook , you will learn the basic of XGBoost , how to apply it on a dataset and tune it to obtain the best performances. * In the second notebook , we will focus on ensemble methods and explain what makes XGBoost different from other models. * Finally in the last notebook you will see how the choice of hyperparameters is a key element of a tradeoff between Bias and Variance . Notebook 1: Introduction to XGBoost Notebook 2: XGBoost and ensemble models Notebook 3: Regularization","title":"Gradient Boosting and XGBoost"},{"location":"xgboost.html#references","text":"XGBoost","title":"References"}]}