{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc-sa/4.0/\"><img alt=\"Creative Commons License\" align=\"left\" src=\"https://i.creativecommons.org/l/by-nc-sa/4.0/80x15.png\" /></a>&nbsp;| [Emmanuel Rachelson](https://personnel.isae-supaero.fr/emmanuel-rachelson?lang=en) | <a href=\"https://supaerodatascience.github.io/machine-learning/\">https://supaerodatascience.github.io/machine-learning/</a>\n",
    "\n",
    "<div style=\"font-size:22pt; line-height:25pt; font-weight:bold; text-align:center;\">XGBoost<br>Introduction to XGBoost</div>\n",
    "\n",
    "This Practice Course is composed of 3 parts - each part is meant to be done in about 1 hour :\n",
    "* In the **first notebook**, you learned the **basic of XGBoost**, how to apply it on a dataset and tune it to obtain the best performances.\n",
    "* In the **second notebook**, we focused on **ensemble methods**.\n",
    "* Finally in the **last notebook** you will see how the choice of a method (such as XGBoost) is a key element of a tradeoff between **Bias and Variance**. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by a few reminder on the Bias/Variance tradeoff :\n",
    "\n",
    "\n",
    "**Bias** is the mean error between our prediction and the correct value. A high bias means that our model makes large mistakes, even on the training dataset.\n",
    "\n",
    "**Variance** is the sensitivity of the prediction to small changes in the dataset. A high variance means that the model performs well on the training dataset, but poorly on the test dataset.\n",
    "\n",
    "Let's summarize that with a figure :\n",
    "\n",
    "<img src=\"img/1 xwtSpR_zg7j7zusa4IDHNQ.png\">\n",
    "\n",
    "In this figure, the center of the target is what we try to predict - as if we were trying to send an arrow in the bullseye zone.\n",
    "A model with a high bias cannot reach the center - the center of gravity of all its arrows is on the outer part of the target.\n",
    "A model with a low bias is globally centered on the target.\n",
    "\n",
    "A model with a high variance is not consistant - its arrows are largely spread.\n",
    "A model with low variance is very consistant - even when its arrows are not in the right place, they are in a limited zone.\n",
    "\n",
    "In a machine learning algorithm, it is a *tradeoff* between Bias and Variance : we want to minimize both (low bias, low variance), but usually when one decreases the other increases.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To control overfitting in Gradient Boosted Trees, we can use 3 main leverages : tree structure, shrinkage and randomization.\n",
    "\n",
    "We already saw that the tree parameters, especially those related to tree depth such as max_depth and gamma, is a way to control the complexity of the model, and therefore the tradeoff bias/variance.\n",
    "\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "    <b>Exercice 1</b><br>\n",
    "      Complete the following code to compare the influence of the number of tree on both the bias and the variance. Complete the annotations.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split, cross_val_predict\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def ground_truth(x):\n",
    "    \"\"\"Ground truth -- function to approximate\"\"\"\n",
    "    return x * np.sin(x) + np.sin(2 * x)\n",
    "\n",
    "def gen_data(n_samples=200):\n",
    "    \"\"\"generate training and testing data\"\"\"\n",
    "    np.random.seed(13)\n",
    "    x = np.random.uniform(0, 10, size=n_samples)\n",
    "    x.sort()\n",
    "    y = ground_truth(x) + 0.75 * np.random.normal(size=n_samples)\n",
    "    train_mask = np.random.randint(0, 2, size=n_samples).astype(np.bool)\n",
    "    x_train, y_train = x[train_mask, np.newaxis], y[train_mask]\n",
    "    x_test, y_test = x[~train_mask, np.newaxis], y[~train_mask]\n",
    "    return x_train, x_test, y_train, y_test\n",
    "\n",
    "X_train, X_test, y_train, y_test = gen_data(200)\n",
    "\n",
    "fig = plt.figure(figsize=(8, 5))\n",
    "ax = plt.gca()\n",
    "\n",
    "test_list = []\n",
    "train_list = []\n",
    "estim_list = []\n",
    "\n",
    "for n_estimators in range(1,1000):\n",
    "    estim_list.append(n_estimators)\n",
    "    ...\n",
    "    \n",
    "    \n",
    "ax.plot(estim_list, test_list, label='Test',\n",
    "         linewidth=2)\n",
    "ax.plot(estim_list,train_list, label='Train', linewidth=2)\n",
    "ax.set_ylabel('Error')\n",
    "ax.set_xlabel('n_estimators')\n",
    "ax.set_ylim((0, 2))\n",
    "\n",
    "ax.annotate('... bias', xy=(900, train_list[900]), xycoords='data',\n",
    "            xytext=(600, 0.3), textcoords='data',\n",
    "            arrowprops=dict(arrowstyle=\"->\", connectionstyle=\"arc\"),\n",
    "            )\n",
    "ax.annotate('... variance', xy=(900, test_list[900]), xycoords='data',\n",
    "            xytext=(600, 0.4), textcoords='data',\n",
    "            arrowprops=dict(arrowstyle=\"->\", connectionstyle=\"arc\"),\n",
    "            )\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shrinkage\n",
    "\n",
    "One of the most usefull technic for regularization in Gradient Boosted Trees is the Shrinkage : the idea is to slow down the training by reducing the prediction of each tree by a scalar - the learning rate. In this way, the model must produce stronger concepts. \n",
    "\n",
    "A low learning rate impose a greater number of trees (n_estimators) to have a similar error during training - we are trading training time for some more precision.\n",
    "\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "    <b>Exercice 2</b><br>\n",
    "      Reuse and modify the previous code to compare two differente learning rates : 1.0 and 0.1. Conclude by completing the annotations.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig = plt.figure(figsize=(8, 5))\n",
    "ax = plt.gca()\n",
    "\n",
    "test_list = []\n",
    "train_list = []\n",
    "estim_list = []\n",
    "\n",
    "test_list_2 = []\n",
    "train_list_2 = []\n",
    "\n",
    "for n_estimators in range(1,1000):\n",
    "    estim_list.append(n_estimators)\n",
    "    est = ...\n",
    "    \n",
    "    est2 = ...\n",
    "    \n",
    "    \n",
    "ax.plot(estim_list, test_list_2, label='Test 0.1',\n",
    "         linewidth=2)\n",
    "ax.plot(estim_list,train_list_2, label='Train 0.1', linewidth=2)\n",
    "\n",
    "ax.plot(estim_list, test_list, label='Test 1.0',\n",
    "         linewidth=2)\n",
    "ax.plot(estim_list,train_list, label='Train 1.0', linewidth=2)\n",
    "ax.set_ylabel('Error')\n",
    "ax.set_xlabel('n_estimators')\n",
    "ax.set_ylim((0, 2))\n",
    "\n",
    "\n",
    "\n",
    "ax.annotate('Requires ... trees', xy=(200, train_list_2[199]), xycoords='data',\n",
    "            xytext=(300, 1.0), textcoords='data',\n",
    "            arrowprops=dict(arrowstyle=\"->\", connectionstyle=\"arc\"),\n",
    "            )\n",
    "ax.annotate('... test error', xy=(900, test_list_2[899]), xycoords='data',\n",
    "            xytext=(600, 0.5), textcoords='data',\n",
    "            arrowprops=dict(arrowstyle=\"->\", connectionstyle=\"arc\"),\n",
    "            )\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Boosting\n",
    "\n",
    "As for many other algorithms, introducing randomness during training can lead to a greater precision at the end. There are two main parameters that introduce randomness :\n",
    "* We can subsample the training data before growing each tree\n",
    "* We can subsample the features before choosing the best split node\n",
    "\n",
    "[XGBoost list of parameters](https://xgboost.readthedocs.io/en/latest/parameter.html)\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "    <b>Exercice 3</b><br>\n",
    "      In XGBoost list of parameters, find the one that control randomness during training.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Sources :\n",
    "* https://xgboost.readthedocs.io/en/latest/tutorials/model.html\n",
    "* https://xgboost.readthedocs.io/en/latest/tutorials/param_tuning.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": false,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
