{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc-sa/4.0/\"><img alt=\"Creative Commons License\" align=\"left\" src=\"https://i.creativecommons.org/l/by-nc-sa/4.0/80x15.png\" /></a>&nbsp;| [Emmanuel Rachelson](https://personnel.isae-supaero.fr/emmanuel-rachelson?lang=en) | <a href=\"https://supaerodatascience.github.io/machine-learning/\">https://supaerodatascience.github.io/machine-learning/</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-size:22pt; line-height:25pt; font-weight:bold; text-align:center;\">Bayesian models for Machine Learning<br>Naive Bayes Classification</div>\n",
    "\n",
    "One very common application of naive Bayes classifiers is document classification (e-mail spam filtering, sentiment analysis on social networks, technical documentation classification, customer appreciations, etc.). \n",
    "\n",
    "Naive Bayes classifiers for documents estimate the probability of a given document belonging to a certain class Y of documents, based on the document's contents Xi.\n",
    "\n",
    "\n",
    "Suppose we want to predict the probability that sample $x$ has label $y$. This is a probability estimation problem that can be written:\n",
    "$$\\mathbb{P}(Y=y|X=x)$$\n",
    "\n",
    "According to Bayes' theorem, we have:\n",
    "$$\\mathbb{P}(Y=y|X=x) =\\frac{\\mathbb{P}(X=x|Y=y)\\cdot\\mathbb{P}(Y=y)}{\\mathbb{P}(X=x)}$$\n",
    "$$\\textrm{posterior} = \\frac{\\textrm{likelihood}\\cdot\\textrm{prior}}{\\textrm{evidence}}$$\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "    \n",
    "**Bayesian inference** is the problem of estimating this **posterior distribution**.<br>\n",
    "In plain words, it consists in estimating the probability of label $y$, given an input $x$, using previously seen data to estimate the **likelihood** of an $x$ input associated to label $y$ and the general **prior** probability of observing label $y$.\n",
    "</div>\n",
    "\n",
    "Note that Bayesian inference applies both to classification and regression.\n",
    "\n",
    "The goal of Bayesian inference is to estimate the label distribution for a given $x$ and use them to predict the correct label, so it is a *probabilistic approach to Machine Learning*.\n",
    "\n",
    "The Bayesian predictor (classifier or regressor) returns the label that maximizes the posterior probability distribution.\n",
    "\n",
    "In this (first) notebook on Bayesian modeling in ML, we will explore the method of Naive Bayes Classification.\n",
    "\n",
    "1. [The naive Bayes assumption](#sec1)\n",
    "2. [Naive Bayes classifiers in scikit-learn](#sec2)\n",
    "3. [Examples](#sec3)\n",
    "    1. [The \"spam or ham?\" example](#sec3-1)\n",
    "    2. [The NIST example](#sec3-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. <a id=\"sec1\"></a>The naive Bayes assumption\n",
    "\n",
    "Let's start with some illustrative data. We consider an artificial data set of 9 individuals. The first column in our data set is the sex ($S=0$ for male, 1 for female), the second is the height $H$ (in meters), the third is the weight $W$ (in kilos) and the last is the foot size $F$ (in centimeters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.  ,  1.82, 82.  , 30.  ],\n",
       "       [ 0.  ,  1.8 , 86.  , 28.  ],\n",
       "       [ 0.  ,  1.7 , 77.  , 30.  ],\n",
       "       [ 0.  ,  1.8 , 75.  , 25.  ],\n",
       "       [ 1.  ,  1.52, 45.  , 15.  ],\n",
       "       [ 1.  ,  1.65, 68.  , 20.  ],\n",
       "       [ 1.  ,  1.68, 59.  , 18.  ],\n",
       "       [ 1.  ,  1.75, 68.  , 23.  ],\n",
       "       [ 1.  ,  1.58, 49.  , 19.  ]])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "fig_size=(10, 10)\n",
    "\n",
    "data = np.loadtxt(\"sex_classif.csv\")\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions** :\n",
    "- Using matplotlib, bokeh, seaborn or plotly, plot one relevant figure on the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Separate data by gender and plot with different colors\n",
    "men = data[data[:, 0] == 0]\n",
    "women = data[data[:, 0] == 1]\n",
    "\n",
    "plt.scatter(men[:, 1], men[:, 2], c='blue', label='Man')\n",
    "plt.scatter(women[:, 1], women[:, 2], c='red', label='Woman')\n",
    "\n",
    "# Customize the plot as needed (title, axis labels, etc.)\n",
    "plt.title(\"Scatter Plot of Data\")\n",
    "plt.xlabel(\"Size\")\n",
    "plt.ylabel(\"Weight\")\n",
    "\n",
    "# Add a legend\n",
    "plt.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would like to answer the question: is $(H=1.81, W=59, F=21)$ male or female?\n",
    "\n",
    "Let's try to estimate $\\mathbb{P}(S=0|H=1.81, W=59, F=21)$.\n",
    "\n",
    "According to Bayes' theorem, the probability that a person that measures 1.81m, weights 59kgs and has a foot size of 21cm is male, is actually the likelihood of observing a person with such features among males, multiplied by the probability of observing males in the population, divided by the probability of observing an individual with these features.\n",
    "\n",
    "That's a long sentence. Let's write that mathematically:\n",
    "$$\\mathbb{P}(S=0|H=1.81, W=59, F=21) = \\frac{\\mathbb{P}(H=1.81, W=59, F=21 | S=0)\\cdot \\mathbb{P}(S=0)}{\\mathbb{P}(H=1.81, W=59, F=21)}$$\n",
    "\n",
    "Let's make that more readable and more general:\n",
    "$$\\mathbb{P}(S|H, W, F) = \\frac{\\mathbb{P}(H,W,F | S)\\cdot \\mathbb{P}(S)}{\\mathbb{P}(H,W,F)}$$\n",
    "\n",
    "Interestingly, since our goal is only to compare the probabilities for $S=0$ and $S=1$, the denominator in the last equation won't be relevant. So we are left with two terms to estimate, given the available data:\n",
    "- $\\mathbb{P}(S=0)$: the prior - the probability that any individual is $S=0$, regardless of his/her physical attributes;\n",
    "- $\\mathbb{P}(H=1.81, W=59, F=21 | S=0)$: the likelihood of meeting somebody with the specified features, given that his/her sex is $S=0$.\n",
    "\n",
    "The prior, in this case, is easy to estimate by comparing the frequencies of male and female individuals in the population.\n",
    "\\begin{gather*}\n",
    "\\mathbb{P}(S=0) = \\frac{4}{9}\\\\\n",
    "\\mathbb{P}(S=1) = \\frac{5}{9}\n",
    "\\end{gather*}\n",
    "Technically, the estimate above is obtained by [*maximum likelihood estimation*](https://en.wikipedia.org/wiki/Maximum_likelihood_estimation).\n",
    "\n",
    "The likelihood, however, is a bit trickier. Can we directly estimate the **joint probability** of the 3 variables $(H,W,F)$?\n",
    "\n",
    "Theoretically, we can. We can assume that among male individuals, $(H,W,F)$ are distributed according to a multivariate Normal distribution, with mean $\\mu=(\\mu_H, \\mu_W, \\mu_F)$ and covariance matrix $\\Sigma$. The trick is then to estimate $\\mu$ and $\\Sigma$.\n",
    "\n",
    "As a matter of fact, estimating $\\mu$ and $\\Sigma$ without further hypothesis would require quite a lot of data, especially because $\\Sigma$ captures the **correlation** between $H$, $W$ and $F$.\n",
    "\n",
    "$\\Sigma$ is a $3\\times 3$ matrix, so it involves 9 parameters to estimate, and we unfortunately only have 9 data points.\n",
    "\n",
    "Let's rephrase this from another perspective. With some basic probabilities, we have:\n",
    "\\begin{align*}\n",
    "\\mathbb{P}(H,W,F | S) = &\\mathbb{P}(H | S)\\\\\n",
    "& \\cdot \\mathbb{P}(W | S, H) \\\\\n",
    "& \\cdot \\mathbb{P}(F | S, H, W)\n",
    "\\end{align*}\n",
    "\n",
    "Those three probabilities are univariate probabilities, much easier to estimate. However, the first one is a function of $S$ only, the second one depends on $S$ and $H$ and the third one depends on $S$, $H$ and $W$. To get an accurate estimate of the third one, we would need samples of the distribution of $F$ in enough points in the space of $(S,H,W)$ to cover it reasonably. This would require a number of data points that is exponential in the number of variables. That's what is called the **curse of dimensionality**, which makes this estimation problem difficult.\n",
    "\n",
    "Let's make this concrete. Suppose we discretize $H$, $W$ and $F$ in 10 bins each and suppose we require 100 samples to get a correct estimate of $\\mathbb{P}(F | S, H, W)$ for any given value of $(F, S, H, W)$. Then we need $100\\cdot 10^3\\cdot 2$ samples to correctly estimate this probability for all possible values of $(F, S, H, W)$. More generally, if we had $n$ continuous features rather than just three, we would require a number of data points that is exponential in $n$.\n",
    "\n",
    "To circumvent this problem, we are going to make a very **naive** assumption (hence the name of the method). We are going to assume that the weight, the height and the foot size are totally independent variables, that is the probability that a person be 1.85m is the same whatever his/her weight and foot size.\n",
    "\n",
    "Obviously, this hypothesis is very strong and clearly does not hold is most real-world cases. But we will assume it nonetheless. In this case, the likelihood estimation becomes:\n",
    "\\begin{align*}\n",
    "\\mathbb{P}(H,W,F | S) = &\\mathbb{P}(H | S)\\\\\n",
    "& \\cdot \\mathbb{P}(W | S) \\\\\n",
    "& \\cdot \\mathbb{P}(F | S)\n",
    "\\end{align*}\n",
    "\n",
    "Each of these probabilities now only depends on the label $S$ and is much easier to estimate from the data. This **conditional independence** assumption is called the **naive Bayes hypothesis**. It allow us to give a (very bad) estimate of $\\mathbb{P}(X | Y)$ and hence of $\\mathbb{P}(Y|X)$.\n",
    "\n",
    "$$\\mathbb{P}(S|H, W, F) = \\frac{\\mathbb{P}(H | S)\\cdot \\mathbb{P}(W | S) \\cdot \\mathbb{P}(F | S)\\cdot \\mathbb{P}(S)}{\\mathbb{P}(H, W, F)}$$\n",
    "\n",
    "Or, in our case:\n",
    "\n",
    "$$\\mathbb{P}(S=0|H=1.81, W=59, F=21) = \\frac{\\mathbb{P}(H=1.81 | S=0)\\cdot \\mathbb{P}(W=59 | S=0) \\cdot \\mathbb{P}(F=21 | S=0)\\cdot \\mathbb{P}(S=0)}{\\mathbb{P}(H=1.81, W=59, F=21)}$$\n",
    "\n",
    "The **naive Bayes classifier** is then the classifier that estimates all class probabilities and returns the one with maximum probability.\n",
    "\n",
    "$$f(H, W, F) = \\arg\\max_{s} \\mathbb{P}(S=s|H,W,F) = \\arg\\max_{s} \\mathbb{P}(H|S=s)\\cdot \\mathbb{P}(W|S=s) \\cdot \\mathbb{P}(F|S=s)\\cdot \\mathbb{P}(S=s)$$\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Exercice:**<br>\n",
    "Let's implement a naive Bayes classifier on the data above, just to practice. We will assume that the $\\mathbb{P}(X | S)$ distributions are Gaussians (for $X = H,W,$ or $F$). Compute the scores and probabilities for each sex, for $(H=1.81, W=59, F=21)$.<br>\n",
    "Hint: use the `np.mean` and `np.std` functions to estimate distribution parameters. Use `scipy.stats.norm.pdf` to compute the Gaussian probability density function in a given input.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score male    : 6.981284895980458e-10\n",
      "score female  : 0.0012161942837264688\n",
      "proba male    : 5.740267802562792e-07\n",
      "proba female  : 0.9999994259732197\n"
     ]
    }
   ],
   "source": [
    "# %load solutions/code1.py\n",
    "### WRITE YOUR CODE HERE\n",
    "# If you get stuck, uncomment the line above to load a correction in this cell (then you can execute this code).\n",
    "\n",
    "# Estimate distribution parameters for males\n",
    "dataM = data[data[:,0]==0]\n",
    "mu_HS0 = np.mean(dataM[:,1])\n",
    "std_HS0 = np.std(dataM[:,1])\n",
    "mu_WS0 = np.mean(dataM[:,2])\n",
    "std_WS0 = np.std(dataM[:,2])\n",
    "mu_FS0 = np.mean(dataM[:,3])\n",
    "std_FS0 = np.std(dataM[:,3])\n",
    "pS0 = dataM.shape[0]/data.shape[0]\n",
    "\n",
    "# Estimate distribution parameters for females\n",
    "dataF = data[data[:,0]==1]\n",
    "mu_HS1 = np.mean(dataF[:,1])\n",
    "std_HS1 = np.std(dataF[:,1])\n",
    "mu_WS1 = np.mean(dataF[:,2])\n",
    "std_WS1 = np.std(dataF[:,2])\n",
    "mu_FS1 = np.mean(dataF[:,3])\n",
    "std_FS1 = np.std(dataF[:,3])\n",
    "pS1 = dataF.shape[0]/data.shape[0]\n",
    "\n",
    "# score that (H=1.81,W=59,F=21) is male/female\n",
    "H=1.81\n",
    "W=59\n",
    "F=21\n",
    "from scipy.stats import norm\n",
    "score_M = pS0 * norm.pdf(H,mu_HS0,std_HS0) * norm.pdf(W,mu_WS0,std_WS0) * norm.pdf(F,mu_FS0,std_FS0)\n",
    "score_F = pS1 * norm.pdf(H,mu_HS1,std_HS1) * norm.pdf(W,mu_WS1,std_WS1) * norm.pdf(F,mu_FS1,std_FS1)\n",
    "print(\"score male    :\", score_M)\n",
    "print(\"score female  :\", score_F)\n",
    "print(\"proba male    :\", score_M/(score_M+score_F))\n",
    "print(\"proba female  :\", score_F/(score_M+score_F))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "import numpy as np\n",
    "\n",
    "dataM = data[data[:,0] == 0]\n",
    "dataM = data[data[:,0] == 1]\n",
    "\n",
    "Mmean_H, Mstd_H = np.mean(dataM[:, 1]), np.std(dataM[:, 1])\n",
    "Mmean_W, Mstd_W = np.mean(dataM[:, 2]), np.std(dataM[:, 2])\n",
    "Mmean_F, Mstd_F = np.mean(dataM[:, 3]), np.std(dataM[:, 3])\n",
    "\n",
    "Fmean_H, Fstd_H = np.mean(dataF[:, 1]), np.std(dataF[:, 1])\n",
    "Fmean_W, Fstd_W = np.mean(dataF[:, 2]), np.std(dataF[:, 2])\n",
    "Fmean_F, Fstd_F = np.mean(dataF[:, 3]), np.std(dataF[:, 3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears we will always multiply together values that are smaller than one. The result will quickly become very small. It is a good habit to move to log-scale.\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "**Exercice:**<br>\n",
    "Reuse your code above to compute log scores instead of scores.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log score male:     -21.08261794758859\n",
      "log score female:   -6.712028735399105\n"
     ]
    }
   ],
   "source": [
    "# %load solutions/code2.py\n",
    "### WRITE YOUR CODE HERE\n",
    "# If you get stuck, uncomment the line above to load a correction in this cell (then you can execute this code).\n",
    "\n",
    "log_score_M = np.log(pS0) + norm.logpdf(H,mu_HS0,std_HS0) + norm.logpdf(W,mu_WS0,std_WS0) + norm.logpdf(F,mu_FS0,std_FS0)\n",
    "log_score_F = np.log(pS1) + norm.logpdf(H,mu_HS1,std_HS1) + norm.logpdf(W,mu_WS1,std_WS1) + norm.logpdf(F,mu_FS1,std_FS1)\n",
    "print(\"log score male:    \", log_score_M)\n",
    "print(\"log score female:  \", log_score_F)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion: $(H=1.81,W=59,F=21)$ is most probably female.\n",
    "\n",
    "Let's generalize.\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "    \n",
    "    \n",
    "Given $n$ features $X_i$ and classes $Y$, **naive Bayes classifiers** estimate (from data) the distributions $\\mathbb{P}(Y)$ and $\\mathbb{P}(X_i|Y)$. Then, using Bayes rule and the naive Bayes assumption, they predict the most probable estimated class:\n",
    "\\begin{align*}\n",
    "\\arg\\max_{y} \\mathbb{P}(Y=y|X=x) & = \\arg\\max_{y} \\frac{\\prod\\limits_{i=1}^n \\mathbb{P}(X_i=x_i|Y=y) \\mathbb{P}(Y=y)}{\\mathbb{P}(X=x)}\\\\\n",
    "& = \\arg\\max_{y} \\prod\\limits_{i=1}^n \\mathbb{P}(X_i=x_i|Y=y) \\mathbb{P}(Y=y)\\\\\n",
    "& = \\arg\\max_{y} \\sum\\limits_{i=1}^n \\log\\left(\\mathbb{P}(X_i=x_i|Y=y)\\right) + \\log\\left(\\mathbb{P}(Y=y)\\right)\n",
    "\\end{align*}\n",
    "</div>\n",
    "\n",
    "Note that although it is not compulsory to compute the denominator, it is quite straightforward since:\n",
    "\\begin{align*}\n",
    "\\mathbb{P}(X=x) &= \\sum\\limits_y \\mathbb{P}(X=x|Y=y)\\mathbb{P}(Y=y)\\\\\n",
    "&= \\sum\\limits_y \\prod\\limits_{i=1}^n \\mathbb{P}(X_i=x_i|Y=y) \\mathbb{P}(Y=y) \n",
    "\\end{align*}\n",
    "So it's the sum of the numerator's values for all $y$, so it's just a matter of normalizing the scores obtained.\n",
    "\n",
    "A really nice thing about naive Bayes classifiers is that it is an **online method**, since most univariate probability distributions can be updated incrementally, "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id=\"sec2\"></a> 2. Naive Bayes classifiers in scikit-learn\n",
    "\n",
    "Once again, scikit-learn has a [naive Bayes](http://scikit-learn.org/stable/modules/naive_bayes.html) implementation. It allows three kind of distributions for the $X_i|Y$ variables: Normal (continuous), Bernouilli or Multinomial (discrete).\n",
    "Let's directly use it on our toy dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction:  [1.]\n",
      "Probas:      [[5.73982396e-07 9.99999426e-01]]\n",
      "Log probas:  [[-1.43706671e+01 -5.73982561e-07]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "gnb = GaussianNB()\n",
    "X = data[:,1:]\n",
    "y = data[:,0]\n",
    "gnb.fit(X,y)\n",
    "xtest = np.array([[1.81,59,21]])\n",
    "print(\"Prediction: \", gnb.predict(xtest))\n",
    "print(\"Probas:     \", gnb.predict_proba(xtest))\n",
    "print(\"Log probas: \",gnb.predict_log_proba(xtest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id=\"sec3\"></a> 3. Examples\n",
    "\n",
    "## <a id=\"sec3-1\"></a> 3.1 The \"spam or ham?\" example\n",
    "\n",
    "Let's scale up and apply naive Bayes classification on the ling-spam data. We will assume a multinomial distribution of word $i$ appearing in and email of class $y$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yann/py_env/sdd_env/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:525: UserWarning:\n",
      "\n",
      "The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data loaded\n"
     ]
    }
   ],
   "source": [
    "from sys import path\n",
    "path.append('../2 - Text data preprocessing')\n",
    "import load_spam\n",
    "spam_data = load_spam.spam_data_loader()\n",
    "spam_data.load_data()\n",
    "print(\"data loaded\")\n",
    "\n",
    "Xtrain, ytrain, Xtest, ytest = spam_data.split(2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Exercice:**\n",
    "Use scikit-learn to build a [multinomial naive Bayes classifier](http://scikit-learn.org/stable/modules/naive_bayes.html#multinomial-naive-bayes) on the data above. Estimate its generalization error.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.9227323628219485\n",
      "******************** done!\n",
      "Average generalization score: 0.9273236282194848\n",
      "Standard deviation: 0.009958853159828238\n"
     ]
    }
   ],
   "source": [
    "# %load solutions/code3.py\n",
    "### WRITE YOUR CODE HERE\n",
    "# If you get stuck, uncomment the line above to load a correction in this cell (then you can execute this code).\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "spam_nbc = MultinomialNB()\n",
    "spam_nbc.fit(Xtrain,ytrain)\n",
    "print(\"Score:\", spam_nbc.score(Xtest,ytest))\n",
    "\n",
    "# Compute cross-validation score\n",
    "nb_trials = 20\n",
    "score = []\n",
    "for i in range(nb_trials):\n",
    "    Xtrain, ytrain, Xtest, ytest = spam_data.shuffle_and_split(2000)\n",
    "    spam_nbc = MultinomialNB()\n",
    "    spam_nbc.fit(Xtrain,ytrain);\n",
    "    score += [spam_nbc.score(Xtest,ytest)]\n",
    "    print('*', end='')\n",
    "print(\" done!\")\n",
    "print(\"Average generalization score:\", np.mean(score))\n",
    "print(\"Standard deviation:\", np.std(score))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've trained our model in the Tf-Idf data. Let's see how the model behaves on raw word counts.\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "**Exercice:**\n",
    "Use scikit-learn to build a [multinomial naive Bayes classifier](http://scikit-learn.org/stable/modules/naive_bayes.html#multinomial-naive-bayes) on the raw word counts data below. Estimate its generalization error.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain, ytrain, Xtest, ytest = spam_data.split(2000, feat='wordcount')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.9876819708846585\n",
      "**************************************************************************************************** done!\n",
      "Average generalization score: 0.9884434490481523\n",
      "Standard deviation: 0.003291130363600036\n"
     ]
    }
   ],
   "source": [
    "# %load solutions/code4.py\n",
    "### WRITE YOUR CODE HERE\n",
    "# If you get stuck, uncomment the line above to load a correction in this cell (then you can execute this code).\n",
    "\n",
    "spam_nbc = MultinomialNB()\n",
    "spam_nbc.fit(Xtrain,ytrain)\n",
    "print(\"Score:\", spam_nbc.score(Xtest,ytest))\n",
    "\n",
    "# Compute cross-validation score\n",
    "nb_trials = 100\n",
    "score = []\n",
    "for i in range(nb_trials):\n",
    "    Xtrain, ytrain, Xtest, ytest = spam_data.shuffle_and_split(2000, feat='wordcount')\n",
    "    spam_nbc = MultinomialNB()\n",
    "    spam_nbc.fit(Xtrain,ytrain);\n",
    "    score += [spam_nbc.score(Xtest,ytest)]\n",
    "    print('*', end='')\n",
    "print(\" done!\")\n",
    "print(\"Average generalization score:\", np.mean(score))\n",
    "print(\"Standard deviation:\", np.std(score))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's identify which are the misclassified emails (and find the confusion matrix by the way)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrain\n",
    "Xtrain, ytrain, Xtest, ytest = spam_data.split(2000, feat='wordcount')\n",
    "spam_nbc = MultinomialNB()\n",
    "spam_nbc.fit(Xtrain,ytrain);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False  True False False False False False\n",
      " False False False False False False  True False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False  True False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False  True False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False  True False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False  True False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False  True\n",
      " False  True False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False  True False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False  True False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False  True False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      "  True False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False  True False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False  True False False\n",
      " False False False False False False False False False  True False False\n",
      " False  True False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False  True False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False  True False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False]\n",
      "Misclassified messages indices: [30, 42, 100, 121, 185, 246, 335, 337, 382, 428, 460, 480, 629, 681, 693, 697, 808, 848]\n"
     ]
    }
   ],
   "source": [
    "# Find misclassified examples\n",
    "ypredict = spam_nbc.predict(Xtest)\n",
    "misclass = np.not_equal(ypredict, ytest)\n",
    "Xmisclass = Xtest[misclass,:]\n",
    "ymisclass = ytest[misclass]\n",
    "misclass_indices = [i for i, j in enumerate(misclass) if j == True]\n",
    "print(\"Misclassified messages indices:\", misclass_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix:\n",
      "[[735  11]\n",
      " [  7 140]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "print(\"Confusion matrix:\")\n",
    "print(confusion_matrix(ytest, ypredict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: [ True]\n",
      "email file: ../data/lingspam_public/bare/part7/6-380msg1.txt\n",
      "email is a spam: False\n",
      "Subject: re : 6 . 199 ipa\n",
      "\n",
      "why must this kind of stuff be decided by a vote ? since obviously ipa membership does not equal interest in the matter and never will , all that will be achieved if some of us join the ipa just to force this issue , is that it will be \" packed \" in some other way than the way it is now , but it will never be representative . why not let the invisible hand of the \" market \" of ideas operate freely instead ? as fewer and fewer people use ipa 's made-up symbols , either that organization will become completely irrelevant , or it will make up its own mind to respond to the \" market forces \" , or perhaps some other group will step in and by proposing a system that is manifestly better than anyone else 's , achieve standardization that way . i really think that if a few of the top names in phonetics got together with the editors of a few journals , they could probably come up with something . it could also simply come from anywhere else : if someone were to publish a truly superior system and people started using it . finally , in reality , it seems that certain trends are occurring anyway and that in particular the resistance to the hachek and the use of the corresponding ipa symbols are on the wane . if we can put up with the inconsistencies of american vs . canadian vs . british spelling , we can probably do just as well here . and if we are gonna worry about something , i would worry more about those cases where te same symbols have different commonly used meanings like ' j ' and ' y ' . alexis mr\n",
      "\n",
      "Bag of words representation (82 words in dictionary):\n",
      "{'subject': 1, 'would': 1, 'like': 1, 'people': 2, 'never': 2, 'different': 1, 'come': 2, 'particular': 1, 'phonetics': 1, 'respond': 1, 'also': 1, 'matter': 1, 'issue': 1, 'u': 1, 'way': 3, 'put': 1, 'use': 2, 'simply': 1, 'let': 1, 'system': 2, 'make': 1, 'mind': 1, 'group': 1, 'kind': 1, 'standardization': 1, 'could': 2, 'since': 1, 'anyway': 1, 'spelling': 1, 'well': 1, 'think': 1, 'obviously': 1, 'interest': 1, 'top': 1, 'used': 1, 'completely': 1, 'either': 1, 'together': 1, 'join': 1, 'got': 1, 'hand': 1, 'vote': 1, 'probably': 2, 'publish': 1, 'must': 1, 'really': 1, 'certain': 1, 'market': 2, 'something': 2, 'made': 1, 'membership': 1, 'representative': 1, 'equal': 1, 'better': 1, 'anyone': 1, 'organization': 1, 'become': 1, 'finally': 1, 'corresponding': 1, 'achieve': 1, 'else': 2, 'perhaps': 1, 'worry': 2, 'step': 1, 'someone': 1, 'force': 1, 'operate': 1, 'stuff': 1, 'truly': 1, 'instead': 1, 'anywhere': 1, 'decided': 1, 'invisible': 1, 'commonly': 1, 'te': 1, 'reality': 1, 'freely': 1, 'superior': 1, 'irrelevant': 1, 'resistance': 1, 'manifestly': 1, 'wane': 1}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.40044569, 0.59955431]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check some misclassified mails\n",
    "index = misclass_indices[1]+2000\n",
    "print(\"Prediction:\", spam_nbc.predict(spam_data.word_count[index,:]))\n",
    "spam_data.print_email(index)\n",
    "spam_nbc.predict_proba(spam_data.tfidf[index,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions** :\n",
    "- What are the next steps to improve the results ? To create the moderation system ?\n",
    "- What questions should you ask to the tech company, before working on their data ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"sec3-2\"></a> 3.2 The NIST example\n",
    "\n",
    "We will assume Gaussian distributions for the NIST example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1797, 64)\n",
      "(1797, 8, 8)\n",
      "(1797,)\n",
      "[0 1 2 3 4 5 6 7 8 9]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "digits = datasets.load_digits()\n",
    "print(digits.data.shape)\n",
    "print(digits.images.shape)\n",
    "print(digits.target.shape)\n",
    "print(digits.target_names)\n",
    "\n",
    "X = digits.data\n",
    "y = digits.target\n",
    "Xtrain,Xtest = np.split(X,[1000])\n",
    "ytrain,ytest = np.split(y,[1000])\n",
    "#Xtrain = X[:1000,:]\n",
    "#ytrain = y[:1000]\n",
    "#Xtest = X[1000:,:]\n",
    "#ytest = y[1000:]\n",
    "\n",
    "#print(digits.DESCR)\n",
    "\n",
    "#plt.gray();\n",
    "#plt.matshow(digits.images[0]);\n",
    "#plt.show();\n",
    "#plt.matshow(digits.images[15]);\n",
    "#plt.show();\n",
    "#plt.matshow(digits.images[42]);\n",
    "#plt.show();\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "def shuffle_and_split(X,y,n):\n",
    "    X0,y0 = shuffle(X,y)\n",
    "    Xtrain,Xtest = np.split(X0,[n])\n",
    "    ytrain,ytest = np.split(y0,[n])\n",
    "    return Xtrain, ytrain, Xtest, ytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 64)\n",
      "(1000,)\n",
      "Generalization error: 0.178168130489335\n",
      "Generalization score: 0.821831869510665\n",
      "Confusion matrix:\n",
      "[[76  0  0  0  0  0  0  0  1  0]\n",
      " [ 0 77  0  0  0  0  1  0  6  0]\n",
      " [ 0 10 45  7  0  0  1  0 18  0]\n",
      " [ 0  0  1 63  0  0  0  3  4  2]\n",
      " [ 1  4  0  0 56  0  3  9  2  0]\n",
      " [ 0  0  0  2  1 70  1  7  2  0]\n",
      " [ 0  2  1  0  0  1 85  0  0  0]\n",
      " [ 0  0  1  0  0  1  0 74  1  0]\n",
      " [ 0  4  2  1  0  0  0  2 71  0]\n",
      " [ 1  6  0 16  2  1  0 11  3 38]]\n"
     ]
    }
   ],
   "source": [
    "Xtrain, ytrain, Xtest, ytest  = shuffle_and_split(X,y,1000)\n",
    "\n",
    "print(Xtrain.shape)\n",
    "print(ytrain.shape)\n",
    "digits_nbc = GaussianNB()\n",
    "digits_nbc.fit(Xtrain,ytrain)\n",
    "prediction = digits_nbc.predict(Xtest)\n",
    "#print(\"Training error:\", np.sum(np.not_equal(prediction,ytest))/len(ytest))\n",
    "print(\"Generalization error:\", np.sum(np.not_equal(prediction,ytest))/len(ytest) )\n",
    "print(\"Generalization score:\", digits_nbc.score(Xtest,ytest))\n",
    "print(\"Confusion matrix:\")\n",
    "print(confusion_matrix(ytest, prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************** done!\n",
      "Average generalization score: 0.8448557089084066\n",
      "Standard deviation: 0.02170775882193408\n"
     ]
    }
   ],
   "source": [
    "# Compute cross-validation score\n",
    "nb_trials = 20\n",
    "score = []\n",
    "for i in range(nb_trials):\n",
    "    Xtrain, ytrain, Xtest, ytest = shuffle_and_split(X,y,1000)\n",
    "    digits_nbc = GaussianNB()\n",
    "    digits_nbc.fit(Xtrain,ytrain)\n",
    "    score += [digits_nbc.score(Xtest,ytest)]\n",
    "    print('*',end='')\n",
    "print(\" done!\")\n",
    "    \n",
    "print(\"Average generalization score:\", np.mean(score))\n",
    "print(\"Standard deviation:\", np.std(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes classifiers reach their limits on data with high correlations between features (like images)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": false,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
